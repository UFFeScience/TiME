{"cells":[{"cell_type":"markdown","metadata":{"id":"k_jJy2MCdD9T"},"source":["#Instalação "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bh4Myg_zc35A"},"outputs":[],"source":["# reading daaset from Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkHpFaw2dLSU"},"outputs":[],"source":["!pip install emoji\n","!pip install simplejson\n","!pip install wmd\n","!pip install kneed\n","\n","#BERTopic\n","!pip install bertopic[all]\n","!pip install umap-learn\n","\n","#Metrics\n","!pip install octis\n","!pip install tmplot\n","!pip install git+https://github.com/maximtrp/tmplot.git\n","!pip install -U contextualized_topic_models\n","\n","#embedding\n","!pip install spacy\n","!python -m spacy download en_core_web_sm\n","!pip install pyyaml==5.4.1\n","!pip install flair"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcjuQq_UdR9P"},"outputs":[],"source":[" # Packages to store and manipulate data\n","import pandas as pd\n","import numpy as np\n","# Example function using numpy:\n","from numpy import dot\n","from numpy.linalg import norm\n","import math\n","import os\n","import json\n","import simplejson as json\n","import csv\n","import string\n","import glob\n","import random\n","import time\n","import math\n","from datetime import datetime\n","pd.set_option('max_colwidth', 200)\n"," \n","# Plotting packages\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","import seaborn as sns\n","import pickle\n","import math\n","import wmd\n","from scipy import spatial\n","from scipy.spatial import distance\n","from scipy.spatial.distance import pdist, squareform\n","from collections import Counter\n","%matplotlib inline\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import tmplot as tmp\n","from tmplot._helpers import get_phi\n","from tmplot._helpers import calc_terms_probs_ratio\n","import umap\n","from umap import UMAP\n"," \n","# Packages\n","import emoji\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import operator\n","from operator import itemgetter\n","from tqdm import tqdm\n","from collections import defaultdict\n","from textblob import TextBlob\n","import sys\n","import itertools\n","import spacy\n","from scipy.sparse import csr_matrix, issparse  # , todense\n","import sys\n","from collections import defaultdict\n","import multiprocessing\n","from pprint import pprint\n","from contextualized_topic_models.evaluation.measures import CoherenceNPMI\n","from hdbscan import HDBSCAN\n"," \n","# Gensim\n","import gensim\n","import gensim.downloader\n","from gensim.test.utils import common_texts\n","from gensim.models import Word2Vec\n","from gensim import models\n","from gensim import corpora\n","from gensim.models import CoherenceModel\n","import gensim.downloader as api\n","from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.models.ldamulticore import LdaMulticore\n"," \n","# Model building package: Sklearn\n","import sklearn\n","from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.datasets import make_multilabel_classification\n","from sklearn.metrics.pairwise import euclidean_distances\n","from sklearn.metrics.pairwise import cosine_distances\n","from sklearn.metrics import pairwise_distances\n","from sklearn.metrics.pairwise import pairwise_kernels\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.manifold import TSNE\n","from sklearn.datasets import load_digits\n","# Cluster documents: Sklearn\n","from kneed import KneeLocator\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import MiniBatchKMeans\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n"," \n","#BERTopic\n","#Embeddings\n","from bertopic import BERTopic\n","from bertopic.backend import WordDocEmbedder\n","from sentence_transformers import SentenceTransformer\n","from flair.embeddings import TransformerDocumentEmbeddings\n","from flair.embeddings import TransformerWordEmbeddings\n","\n","#Metrics\n","from octis.evaluation_metrics.metrics import AbstractMetric\n","from octis.dataset.dataset import Dataset\n","from gensim.corpora.dictionary import Dictionary\n","from gensim.models import CoherenceModel\n","import octis.configuration.citations as citations\n","import numpy as np\n","import itertools\n","from scipy import spatial\n","from sklearn.metrics import pairwise_distances\n","from operator import add\n","from octis.evaluation_metrics.rbo import rbo\n","import gensim.downloader as api\n","from gensim.models import KeyedVectors\n","from octis.evaluation_metrics.diversity_metrics import WordEmbeddingsInvertedRBO, \\\n","    WordEmbeddingsInvertedRBOCentroid, InvertedRBO\n","from itertools import combinations\n","from scipy.spatial.distance import cosine\n","import gensim.downloader as api"]},{"cell_type":"markdown","metadata":{"id":"QXzUzxh8WH05"},"source":["#Importar Dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51wBO3uMdWQU"},"outputs":[],"source":["df = pd.read_json('/content/drive/dados_fevereiro_dia_de_semana_1.json')\n","\n","#Tokenizar\n","new_tweet = []\n","for tweet in df.clean_tweet:\n","    tweet_len = [word for word in tweet.split(' ') if len(word) >= 3]\n","    tweet_string = ' '.join(tweet_len)\n","    new_tweet.append(tweet_string)\n","df['clean_tweet'] = new_tweet\n","\n","stop = stopwords.words('english')\n","df['clean_tweet_tokenize'] = df['clean_tweet'].apply(lambda x: [item for item in str(x).split(' ')])\n","df.dropna()"]},{"cell_type":"markdown","metadata":{"id":"2KDufXNaKwcB"},"source":["#Modelagem de Tópicos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EUcIY_wmlO8"},"outputs":[],"source":["# cast tweets to numpy array\n","docs = df.clean_tweet_tokenize.to_numpy()\n","\n","# create dictionary of all words in all documents\n","dictionary =  corpora.Dictionary(docs)\n","\n","# create variable containing length of dictionary/vocab\n","vocab_length = len(dictionary)\n","\n","# create BOW dictionary\n","bow_corpus = [dictionary.doc2bow(doc) for doc in docs]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XifW48uAMpC"},"outputs":[],"source":["#UMAP\n","umap_model = UMAP(n_neighbors=15, n_components=5, \n","                  min_dist=0.0, metric='cosine', random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2A61LNPpLlA"},"outputs":[],"source":["# Word embedding\n","embedding_word_2 = TransformerWordEmbeddings('roberta-large')\n","\n","# Document embedding model\n","embedding_document_4 = SentenceTransformer('all-mpnet-base-v2')\n","\n","# Create a model that uses both language models and pass it through BERTopic\n","word_doc_embedder = WordDocEmbedder(embedding_model=embedding_document_4, word_embedding_model=embedding_word_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQjua6GGTFo5"},"outputs":[],"source":["topic_model = BERTopic(nr_topics= \"auto\", language='english', calculate_probabilities=True, embedding_model=word_doc_embedder, umap_model=umap_model, verbose=True).fit(list(df.clean_tweet))\n","topics, probabilities = topic_model.fit_transform(list(df.clean_tweet))"]},{"cell_type":"code","source":["# Save model\n","topic_model.save(\"model_train\")\t\n","# Load model\n","topic_model_test = BERTopic.load(\"model_train\")\t"],"metadata":{"id":"7VQU39y0brhv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_I1PuUwRUgP6"},"outputs":[],"source":["nr_topics = 10\n","new_topics, new_probs = topic_model.reduce_topics(df.clean_tweet, topics, probabilities, nr_topics=nr_topics)\n","freq = topic_model.get_topic_info();"]},{"cell_type":"markdown","metadata":{"id":"RlH7yf25YnXS"},"source":["#Tópicos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CMFumppUTqo"},"outputs":[],"source":["topic = pd.DataFrame()\n","topics_train = []\n","for i in range(1, nr_topics + 1):\n","  top = []\n","  name = 'Topic '+ str(i - 1)\n","  topic_nr = freq.iloc[i][\"Topic\"]\n","  for j in range(len(topic_model.get_topic(topic_nr))):\n","    top.append(topic_model.get_topic(topic_nr)[j][0])\n","  topics_train.append(top)\n","  topic[name] = top\n","topic "]},{"cell_type":"markdown","metadata":{"id":"oWOBEO3gVTlH"},"source":["#Métricas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfBkrBZ4Vre2"},"outputs":[],"source":["cm_bertopic = CoherenceModel(topics=topics_train, \n","                          dictionary=dictionary, \n","                          corpus=bow_corpus, \n","                          texts=docs, \n","                          coherence='c_v')\n","\n","print(cm_bertopic.get_coherence()) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAHXfVWLVYiI"},"outputs":[],"source":["npmi = CoherenceModel(topics=topics_train, \n","                          dictionary=dictionary, \n","                          corpus=bow_corpus, \n","                          texts=docs,\n","                          coherence='c_npmi')\n","\n","print(npmi.get_coherence())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"am3OWQUJ1YrZ"},"outputs":[],"source":["def get_word2index(list1, list2):\n","    words = set(list1)\n","    words = words.union(set(list2))\n","    word2index = {w: i for i, w in enumerate(words)}\n","    return word2index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljV2sN8eYxyt"},"outputs":[],"source":["#Diversity Metrics:\n","class TopicDiversity(AbstractMetric):\n","    def __init__(self, topk=10):\n","        \"\"\"\n","        Initialize metric\n","        Parameters\n","        ----------\n","        topk: top k words on which the topic diversity will be computed\n","        \"\"\"\n","        AbstractMetric.__init__(self)\n","        self.topk = topk\n","\n","    def score(self, model_output):\n","        \"\"\"\n","        Retrieves the score of the metric\n","        Parameters\n","        ----------\n","        model_output : dictionary, output of the model\n","                       key 'topics' required.\n","        Returns\n","        -------\n","        td : score\n","        \"\"\"\n","        topics = model_output\n","        if topics is None:\n","            return 0\n","        if self.topk > len(topics[0]):\n","            raise Exception('Words in topics are less than ' + str(self.topk))\n","        else:\n","            unique_words = set()\n","            for topic in topics:\n","                unique_words = unique_words.union(set(topic[:self.topk]))\n","            td = len(unique_words) / (self.topk * len(topics))\n","            return td"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MNdaROvxjGg"},"outputs":[],"source":["class InvertedRBO(AbstractMetric):\n","    def __init__(self, topk=10, weight=0.9):\n","        \"\"\"\n","        Initialize metric Inverted Ranked-Biased Overlap\n","        :param topk: top k words on which the topic diversity will be computed\n","        :param weight: weight of each agreement at depth d. When set to 1.0, there is no weight, the rbo returns to\n","        average overlap. (default 0.9)\n","        \"\"\"\n","        super().__init__()\n","        self.topk = topk\n","        self.weight = weight\n","\n","    def score(self, model_output):\n","        \"\"\"\n","        Retrieves the score of the metric\n","        :param model_output : dictionary, output of the model. the 'topics' key is required.\n","        \"\"\"\n","        topics = model_output\n","        if topics is None:\n","            return 0\n","        if self.topk > len(topics[0]):\n","            raise Exception('Words in topics are less than topk')\n","        else:\n","            collect = []\n","            for list1, list2 in itertools.combinations(topics, 2):\n","                word2index = get_word2index(list1, list2)\n","                indexed_list1 = [word2index[word] for word in list1]\n","                indexed_list2 = [word2index[word] for word in list2]\n","                rbo_val = rbo(indexed_list1[:self.topk], indexed_list2[:self.topk], p=self.weight)[2]\n","                collect.append(rbo_val)\n","            return 1 - np.mean(collect)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8Gz-3PT1sxc"},"outputs":[],"source":["#Similarity Metrics:\n","class PairwiseJaccardSimilarity(AbstractMetric):\n","    def __init__(self, topk=10):\n","        \"\"\"\n","        Initialize metric Pairwise Jaccard Similarity\n","        Parameters\n","        ----------\n","        :param topk: top k words on which the topic diversity will be computed\n","        \"\"\"\n","        super().__init__()\n","        self.topk = topk\n","\n","    def score(self, model_output):\n","        \"\"\"\n","        Retrieves the score of the metric\n","        :return PJS\n","        \"\"\"\n","        topics = model_output\n","        sim = 0\n","        count = 0\n","        for list1, list2 in combinations(topics, 2):\n","            intersection = len(list(set(list1[:self.topk]).intersection(list2[:self.topk])))\n","            union = (len(list1[:self.topk]) + len(list2[:self.topk])) - intersection\n","            count = count + 1\n","            sim = sim + (float(intersection) / union)\n","        return sim / count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edPyABcRY0rr"},"outputs":[],"source":["diversity = TopicDiversity()\n","diversity.score(topics_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAqp1p3QZADQ"},"outputs":[],"source":["b = InvertedRBO()\n","b.score(topics_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLHufrTkd45Z"},"outputs":[],"source":["jaccard = PairwiseJaccardSimilarity()\n","jaccard.score(topics_train)"]},{"cell_type":"markdown","metadata":{"id":"ZT7LbB_JAgSE"},"source":["##Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sf01JdqUGYy7"},"outputs":[],"source":["representative_docs = topic_model.get_representative_docs(5)\n","representative_docs #Para obter os documentos representativos de um único tópico"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBWLpM_XiHEW"},"outputs":[],"source":["fig = topic_model.visualize_topics(); fig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T-x1jT-6i1SV"},"outputs":[],"source":["topics_over_time = topic_model.topics_over_time(docs=df.clean_tweet, \n","                                                topics=new_topics, \n","                                                timestamps=df.created_at, \n","                                                global_tuning=True, \n","                                                evolution_tuning=True, \n","                                                nr_bins=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0lgEMdpBi1uR"},"outputs":[],"source":["topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Cy8dvW3CDmE"},"outputs":[],"source":["topic_model.visualize_hierarchy(top_n_topics=11)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTxjjsF-CHaP"},"outputs":[],"source":["topic_model.visualize_barchart(top_n_topics=11)"]}],"metadata":{"colab":{"provenance":[{"file_id":"11NMi91V73OKTPXQKV1PlCuecYsvJezJf","timestamp":1662137726855},{"file_id":"1Hzml-D2ubkBDlfU9kP5EXSLiG-9-7AoS","timestamp":1649937045425},{"file_id":"1W_AZE6Pf4n2RtYhqnD6aTi2yM9ieM00d","timestamp":1649725484504},{"file_id":"1Nn1r2iIy6fGKO3y5Ivz9zfoCjJffViu0","timestamp":1643386890127},{"file_id":"1SYRVAF1z9ei5D25OM5s_W7a8Ra5vVFC-","timestamp":1643132887929}],"authorship_tag":"ABX9TyNSYhnsgcWkVV1+lzochFNf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}