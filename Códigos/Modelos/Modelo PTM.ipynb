{"cells":[{"cell_type":"markdown","metadata":{"id":"k_jJy2MCdD9T"},"source":["#Instalação "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bh4Myg_zc35A"},"outputs":[],"source":["# reading daaset from Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkHpFaw2dLSU"},"outputs":[],"source":["!pip install emoji\n","!pip install simplejson\n","!pip install wmd\n","!pip install kneed\n","#PTM\n","!pip install --upgrade pip\n","!pip install tomotopy\n","#Metrics\n","!pip install octis\n","\n","!pip install tmplot\n","!pip install git+https://github.com/maximtrp/tmplot.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcjuQq_UdR9P"},"outputs":[],"source":[" # Packages to store and manipulate data\n","import pandas as pd\n","import numpy as np\n","# Example function using numpy:\n","from numpy import dot\n","from numpy.linalg import norm\n","import math\n","import os\n","import json\n","import simplejson as json\n","import csv\n","import string\n","import glob\n","import random\n","import time\n","import math\n","from datetime import datetime\n","pd.set_option('max_colwidth', 200)\n"," \n","# Plotting packages\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","import seaborn as sns\n","import pickle\n","import math\n","import wmd\n","from scipy import spatial\n","from scipy.spatial import distance\n","from scipy.spatial.distance import pdist, squareform\n","from collections import Counter\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import tmplot as tmp\n","from tmplot._helpers import get_phi\n","from tmplot._helpers import calc_terms_probs_ratio\n"," \n","# Packages\n","import emoji\n","import re\n","import nltk\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import operator\n","from operator import itemgetter\n","from tqdm import tqdm\n","from collections import defaultdict\n","from textblob import TextBlob\n","import sys\n","import itertools\n","import spacy\n","from scipy.sparse import csr_matrix, issparse  # , todense\n","import sys\n","from collections import defaultdict\n","import multiprocessing\n","from pprint import pprint\n"," \n","# Gensim\n","import gensim\n","import gensim.downloader\n","from gensim.test.utils import common_texts\n","from gensim.models import Word2Vec\n","from gensim import models\n","from gensim import corpora\n","from gensim.models import CoherenceModel\n","import gensim.downloader as api\n","from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.models.ldamulticore import LdaMulticore\n"," \n","# Model building package: Sklearn\n","import sklearn\n","from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.datasets import make_multilabel_classification\n","from sklearn.metrics.pairwise import euclidean_distances\n","from sklearn.metrics.pairwise import cosine_distances\n","from sklearn.metrics import pairwise_distances\n","from sklearn.metrics.pairwise import pairwise_kernels\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.manifold import TSNE\n","from sklearn.datasets import load_digits\n","# Cluster documents: Sklearn\n","from kneed import KneeLocator\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import MiniBatchKMeans\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n","\n","#PTM\n","import tomotopy as tp\n","\n","#Metrics\n","from octis.evaluation_metrics.metrics import AbstractMetric\n","from octis.dataset.dataset import Dataset\n","from gensim.corpora.dictionary import Dictionary\n","from gensim.models import CoherenceModel\n","import octis.configuration.citations as citations\n","import numpy as np\n","import itertools\n","from scipy import spatial\n","from sklearn.metrics import pairwise_distances\n","from operator import add\n","from octis.evaluation_metrics.rbo import rbo\n","import gensim.downloader as api\n","from gensim.models import KeyedVectors\n","from octis.evaluation_metrics.diversity_metrics import WordEmbeddingsInvertedRBO, \\\n","    WordEmbeddingsInvertedRBOCentroid, InvertedRBO\n","from itertools import combinations\n","from scipy.spatial.distance import cosine"]},{"cell_type":"markdown","metadata":{"id":"QXzUzxh8WH05"},"source":["#Importar Dados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51wBO3uMdWQU"},"outputs":[],"source":["df = pd.read_json('/content/drive/dados_fevereiro_dia_de_semana_1.json')\n","\n","#Tokenizar\n","new_tweet = []\n","for tweet in df.clean_tweet:\n","    tweet_len = [word for word in tweet.split(' ') if len(word) >= 3]\n","    tweet_string = ' '.join(tweet_len)\n","    new_tweet.append(tweet_string)\n","df['clean_tweet'] = new_tweet\n","\n","stop = stopwords.words('english')\n","df['clean_tweet_tokenize'] = df['clean_tweet'].apply(lambda x: [item for item in str(x).split(' ')])\n","df.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhiI98WEHEJO"},"outputs":[],"source":["d_clean = df.clean_tweet\n","d_clean.to_csv(\"clean_tweet.csv\")"]},{"cell_type":"markdown","metadata":{"id":"0VqdvGqIbEll"},"source":["#Modelagem de Tópicos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EUcIY_wmlO8"},"outputs":[],"source":["# cast tweets to numpy array\n","docs = df.clean_tweet_tokenize.to_numpy()\n","\n","# create dictionary of all words in all documents\n","dictionary =  corpora.Dictionary(docs)\n","\n","# create BOW dictionary\n","bow_corpus = [dictionary.doc2bow(doc) for doc in docs]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaHtt8Bua_xK"},"outputs":[],"source":["mdl = tp.PTModel(k=10,seed=42)  \n","for n, line in enumerate(open(\"clean_tweet.csv\", encoding='utf-8')):\n","    ch = line.strip().split()\n","    mdl.add_doc(ch)\n","mdl.burn_in = 100\n","mdl.train(0)\n","for i in range(0, 1000, 10):\n","    mdl.train(10)\n","mdl.summary()"]},{"cell_type":"code","source":["# save into file\n","mdl.save('model_train.bin')\n","# load from file\n","mdl_test = tp.PTModel.load('model_train.bin')"],"metadata":{"id":"SNZXsHk3a8M4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Tópicos"],"metadata":{"id":"cyi-l9bECA9F"}},{"cell_type":"code","source":["print('Num docs:', len(mdl.docs), ', Vocab size:', len(mdl.used_vocabs), ', Num words:', mdl.num_words)\n","print('Removed top words:', mdl.removed_top_words)\n","print('Training...', file=sys.stderr, flush=True)"],"metadata":{"id":"nJejqZ1RCMOE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topics = []\n","for k in range(mdl.k):\n","  l_topic = []\n","  for word, prob in mdl.get_topic_words(k):\n","    l_topic.append(word)\n","  topics.append(l_topic)"],"metadata":{"id":"sytvrZOiZxVf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topic = pd.DataFrame()\n","for i in range(k+1):\n","  topic['Topic '+str(i)] = topics[i]\n","topic "],"metadata":{"id":"p6zDoBiRTPA3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Metrics"],"metadata":{"id":"oWOBEO3gVTlH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8lOsU4opN2ZH"},"outputs":[],"source":["# cast tweets to numpy array\n","docs = df.clean_tweet_tokenize.to_numpy()\n","\n","# create dictionary of all words in all documents\n","dictionary =  corpora.Dictionary(docs)\n","\n","# create BOW dictionary\n","bow_corpus = [dictionary.doc2bow(doc) for doc in docs]"]},{"cell_type":"code","source":["cm_ptm = CoherenceModel(topics=topics, \n","                            dictionary=dictionary, \n","                            corpus=bow_corpus, \n","                            texts=docs, \n","                            coherence='c_v')\n","print(cm_ptm.get_coherence())"],"metadata":{"id":"ZfBkrBZ4Vre2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["npmi = CoherenceModel(topics=topics, \n","                          dictionary=dictionary, \n","                          corpus=bow_corpus, \n","                          texts=docs,\n","                          coherence='c_npmi')\n","\n","print(npmi.get_coherence())"],"metadata":{"id":"AAHXfVWLVYiI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_word2index(list1, list2):\n","    words = set(list1)\n","    words = words.union(set(list2))\n","    word2index = {w: i for i, w in enumerate(words)}\n","    return word2index"],"metadata":{"id":"am3OWQUJ1YrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Diversity Metrics:\n","class TopicDiversity(AbstractMetric):\n","    def __init__(self, topk=10):\n","        \"\"\"\n","        Initialize metric\n","        Parameters\n","        ----------\n","        topk: top k words on which the topic diversity will be computed\n","        \"\"\"\n","        AbstractMetric.__init__(self)\n","        self.topk = topk\n","\n","    def score(self, model_output):\n","        \"\"\"\n","        Retrieves the score of the metric\n","        Parameters\n","        ----------\n","        model_output : dictionary, output of the model\n","                       key 'topics' required.\n","        Returns\n","        -------\n","        td : score\n","        \"\"\"\n","        topics = model_output\n","        if topics is None:\n","            return 0\n","        if self.topk > len(topics[0]):\n","            raise Exception('Words in topics are less than ' + str(self.topk))\n","        else:\n","            unique_words = set()\n","            for topic in topics:\n","                unique_words = unique_words.union(set(topic[:self.topk]))\n","            td = len(unique_words) / (self.topk * len(topics))\n","            return td"],"metadata":{"id":"ljV2sN8eYxyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["diversity = TopicDiversity()\n","diversity.score(topics)"],"metadata":{"id":"edPyABcRY0rr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InvertedRBO(AbstractMetric):\n","    def __init__(self, topk=10, weight=0.9):\n","        \"\"\"\n","        Initialize metric Inverted Ranked-Biased Overlap\n","        :param topk: top k words on which the topic diversity will be computed\n","        :param weight: weight of each agreement at depth d. When set to 1.0, there is no weight, the rbo returns to\n","        average overlap. (default 0.9)\n","        \"\"\"\n","        super().__init__()\n","        self.topk = topk\n","        self.weight = weight\n","\n","    def score(self, model_output):\n","        \"\"\"\n","        Retrieves the score of the metric\n","        :param model_output : dictionary, output of the model. the 'topics' key is required.\n","        \"\"\"\n","        topics = model_output\n","        if topics is None:\n","            return 0\n","        if self.topk > len(topics[0]):\n","            raise Exception('Words in topics are less than topk')\n","        else:\n","            collect = []\n","            for list1, list2 in itertools.combinations(topics, 2):\n","                word2index = get_word2index(list1, list2)\n","                indexed_list1 = [word2index[word] for word in list1]\n","                indexed_list2 = [word2index[word] for word in list2]\n","                rbo_val = rbo(indexed_list1[:self.topk], indexed_list2[:self.topk], p=self.weight)[2]\n","                collect.append(rbo_val)\n","            return 1 - np.mean(collect)"],"metadata":{"id":"-MNdaROvxjGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b = InvertedRBO()\n","b.score(topics)"],"metadata":{"id":"nAqp1p3QZADQ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["k_jJy2MCdD9T","QXzUzxh8WH05"],"provenance":[{"file_id":"18zicbZN6I2v-1MT6bQHvO7IS9Iw03eYs","timestamp":1659457826906},{"file_id":"1hih--6NEY_eHvlTPV2dBzSRcetrrvSzu","timestamp":1650565162632},{"file_id":"1iS0CbP-u0HpnwH6uV7LxN2upBHrkarMZ","timestamp":1649442818609},{"file_id":"1xbaaQdY2JTGV3olQOn5yM46thEYc5h7I","timestamp":1644686102907},{"file_id":"1SYRVAF1z9ei5D25OM5s_W7a8Ra5vVFC-","timestamp":1644610045835}],"authorship_tag":"ABX9TyMwMcvDUEv8zNqe73+zm6JB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}