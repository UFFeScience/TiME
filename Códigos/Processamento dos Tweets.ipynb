{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFvPY6JAPIsc"
      },
      "source": [
        "#Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oQJBdnEmOnVR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gek6WOhgO3_r"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "!pip install simplejson\n",
        "!pip install wmd\n",
        "!pip install kneed\n",
        "!pip install ray\n",
        "!pip install ray[default]\n",
        "!pip install -U ray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UNxmNYeMPAIB"
      },
      "outputs": [],
      "source": [
        "# Packages to store and manipulate data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Example function using numpy:\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import math\n",
        "import os\n",
        "import json\n",
        "import simplejson as json\n",
        "import csv\n",
        "import string\n",
        "import glob\n",
        "import re\n",
        "from datetime import datetime\n",
        "pd.set_option('max_colwidth', 200)\n",
        "\n",
        "# Plotting packages\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns \n",
        "import pickle\n",
        "import math\n",
        "import wmd\n",
        "from scipy import spatial\n",
        "from scipy.spatial import distance\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from collections import Counter\n",
        "import plotly.express as px\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "# Package to clean text\n",
        "import emoji\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import operator\n",
        "from operator import itemgetter\n",
        "from operator import itemgetter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Package NPL\n",
        "import nltk\n",
        "nltk.download('punkt') #This is a library that helps us tokenize words and phrases\n",
        "nltk.download('stopwords')\n",
        "nltk.download ( 'wordnet' ) #We use the data and methods in this library to stem our data\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import brown\n",
        "stop_words = stopwords.words('english')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk import bigrams\n",
        "from nltk.corpus import brown\n",
        "from nltk.util import ngrams\n",
        "from nltk import word_tokenize\n",
        "from nltk import RegexpParser\n",
        "\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from scipy.sparse import csr_matrix, issparse  # , todense\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "import multiprocessing\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "from gensim import models\n",
        "from gensim import corpora\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import gensim.downloader as api\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
        "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "import gensim\n",
        "\n",
        "# Model building package: Sklearn\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.metrics.pairwise import pairwise_kernels\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.datasets import load_digits\n",
        "# Cluster documents: Sklearn\n",
        "from kneed import KneeLocator\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "# Metric distance: Sklearn\n",
        "\n",
        "# T-SNE Clustering Chart \n",
        "# Get topic weights and dominant topics\n",
        "from sklearn.manifold import TSNE\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.models import Label\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "#importing the glove library\n",
        "#from glove import Corpus, Glove\n",
        "\n",
        "3# Ray\n",
        "import ray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ophLZ0EPeje"
      },
      "source": [
        "#Importar e Filtrar o Dataframe dos Tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cf7yqLOfQTm9"
      },
      "outputs": [],
      "source": [
        "#ray.init()\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ND1t3_XAPkKM"
      },
      "outputs": [],
      "source": [
        "@ray.remote\n",
        "class PreprocessingOperation:\n",
        "\n",
        "  def __init__(self, csv_file_path):\n",
        "    self.csv_file_path = csv_file_path\n",
        "    self.dataframe = self.read_csv()\n",
        "   \n",
        "    self.__filter_columns()\n",
        "    self.__filter_by_original_tweets()\n",
        "    self.__convert_date()\n",
        "\n",
        "  def read_csv(self):\n",
        "    return pd.read_csv(self.csv_file_path, error_bad_lines=False, index_col=False, dtype='unicode')\n",
        "\n",
        "  def __filter_columns(self):\n",
        "    self.dataframe = self.dataframe.loc[:,['created_at','id','tweet']]\n",
        "    self.dataframe.drop_duplicates()#inplace=True)\n",
        "    self.dataframe.dropna(inplace=True)\n",
        "\n",
        "  def __filter_by_original_tweets(self):\n",
        "    self.dataframe = self.dataframe.loc[self.dataframe.tweet.apply(lambda x: x[:4]!='RT @')]\n",
        "\n",
        "  def __convert_date(self):\n",
        "    self.dataframe.drop(self.dataframe[ self.dataframe['created_at'] == 'created_at' ].index , inplace=True)\n",
        "    self.dataframe['created_at'] = pd.to_datetime(self.dataframe['created_at']).dt.tz_localize(None) #fuso horário\n",
        "    \n",
        "  def get_dataframe(self):\n",
        "    dataframe = self.dataframe\n",
        "    del self.dataframe\n",
        "    return dataframe\n",
        "\n",
        "file_paths = [     \n",
        "'Clima/data_22_02_12.csv',\n",
        "'Comida/data_22_02_12.csv',\n",
        "'Esporte/data_22_02_12.csv',\n",
        "'Filmes e séries/data_22_02_12.csv',\n",
        "'Jogos/data_22_02_12.csv',\n",
        "'Música/data_22_02_12.csv',\n",
        "'Política/data_22_02_12.csv',\n",
        "'Religião/data_22_02_12.csv',\n",
        "'Romance/data_22_02_12.csv',\n",
        "'Saúde/data_22_02_12.csv']\n",
        "\n",
        "# ray approach --> parallel\n",
        "preprocessing_ops = [PreprocessingOperation.remote(file_path) for file_path in file_paths]\n",
        "ray_dfs = [preprocessing_op.get_dataframe.remote() for preprocessing_op in preprocessing_ops]\n",
        "dataframes = ray.get(ray_dfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hj7Vxk63Qc4t"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=['created_at','id', 'tweet'])\n",
        "for dataframe in dataframes:\n",
        "  dataframe = pd.DataFrame(dataframe,columns=['created_at','id', 'tweet'])\n",
        "  df = pd.concat([df, dataframe])\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTIDDR1MQw2w"
      },
      "source": [
        "#Pré-processamento dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P0vpnnQ3Qk7z"
      },
      "outputs": [],
      "source": [
        "class Processing:\n",
        "    \n",
        "    def __init__(self, dataframe):\n",
        "      self.dataframe = dataframe\n",
        "      self.__remove_links()\n",
        "      self.__remove_user()\n",
        "      self.__remove_hashtag()\n",
        "      self.__lower_tweets()\n",
        "      self.__remove_numbers()\n",
        "      self.__remove_punctuation()\n",
        "      self.__remove_emoji()\n",
        "      #self.__translate_emoji()\n",
        "      self.__remove_short_words_and_stopwords()\n",
        "      self.__convert_to_string()\n",
        "      self.__lower_tweets()\n",
        "      self.__remove_double_spacing()\n",
        "      self.__remove_nan_in_clean_tweet()\n",
        "\n",
        "    def __remove_links(self):\n",
        "      new_tweet = []\n",
        "      for tweet in self.dataframe.tweet:\n",
        "          tweet = re.sub(r'http\\S+', '', tweet) # Remove http links\n",
        "          tweet = re.sub(r'bit.ly/\\S+', '', tweet) # Remove bitly links\n",
        "          tweet = re.sub(r'&amp', '', tweet) # Remove amp\n",
        "          tweet = tweet.strip('[link]') # Remove [links]\n",
        "          new_tweet.append(tweet)\n",
        "      self.dataframe['clean_tweet'] = new_tweet \n",
        "    \n",
        "    def __remove_user(self):\n",
        "      new_tweet = []\n",
        "      for tweet in self.dataframe.clean_tweet:\n",
        "          tweet = re.sub(r'(@[A-Za-z0-9-_]+[A-Za-z0-9-_]+)', '', tweet) # Remove tweeted at\n",
        "          new_tweet.append(tweet)\n",
        "      self.dataframe['clean_tweet'] = new_tweet \n",
        "\n",
        "    def __remove_hashtag(self):\n",
        "      new_tweet = []\n",
        "      for tweet in self.dataframe.clean_tweet:\n",
        "          tweet = re.sub(r'(#[A-Za-z0-9-_]+[A-Za-z0-9-_]+)', '', tweet) # Remove hashtags\n",
        "          new_tweet.append(tweet)\n",
        "      self.dataframe['clean_tweet'] = new_tweet \n",
        "\n",
        "    def __lower_tweets(self):\n",
        "      self.dataframe['clean_tweet'] = self.dataframe['clean_tweet'].apply(lambda x: x.lower())  \n",
        "        \n",
        "    def __remove_punctuation(self):\n",
        "      my_punctuation = '#!\"…¨$%&\\'”’(“)*+,-./:;<=>?[\\\\]^_‘`{|}~•@°'\n",
        "      new_tweet = []\n",
        "      for tweet in self.dataframe.clean_tweet:\n",
        "          tweet = re.sub('['+my_punctuation+']+', ' ', tweet) \n",
        "          new_tweet.append(tweet)\n",
        "      self.dataframe['clean_tweet'] = new_tweet\n",
        "\n",
        "    def __remove_numbers(self):\n",
        "      new_tweet = []\n",
        "      for tweet in self.dataframe.clean_tweet:\n",
        "          tweet = re.sub('([0-9]+)', '', tweet)\n",
        "          tweet = tweet.strip()\n",
        "          new_tweet.append(tweet)\n",
        "      self.dataframe['clean_tweet'] = new_tweet\n",
        "      \n",
        "    def __translate_emoji(self):\n",
        "        new_tweet = []\n",
        "        for tweet in self.dataframe.clean_tweet:\n",
        "            tweet = emoji.demojize(tweet) \n",
        "            tweet = re.sub(r':', ' ',  tweet)\n",
        "            tweet = re.sub(r\"[^a-zA-Z0-9-_]\", ' ', tweet) \n",
        "            new_tweet.append(tweet)\n",
        "        self.dataframe['clean_tweet'] = new_tweet\n",
        "\n",
        "    def __remove_emoji(self):\n",
        "      new_tweet = []\n",
        "      for tweet in self.dataframe.clean_tweet:\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "        tweet = emoji_pattern.sub(r'', tweet)\n",
        "        new_tweet.append(tweet)\n",
        "      self.dataframe['clean_tweet'] = new_tweet\n",
        "\n",
        "    def __remove_short_words_and_stopwords(self):\n",
        "      all_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
        "      new_tweet = []\n",
        "      stop = stopwords.words('english')\n",
        "      stop.extend(['est','be','on','!','at','.',':','...','@',',','#','will','.m','in','a','the','with','to','by','and','my','is',\n",
        "                   'of','for','new','via','are','that','has','have','all','as','it','so','they','do','he','just','this',\n",
        "                   'was','who','your','from','his','about','get','but','am','up','if','can','would','than','should','dont',\n",
        "                   'had','or','were','did','there','got','even','its','an'])\n",
        "      for tweet in self.dataframe.clean_tweet:\n",
        "          tweet_len = [word for word in tweet.split(' ') if len(word)> 2 and (word not in stop and word not in all_stopwords)]\n",
        "          new_tweet.append(tweet_len)\n",
        "      self.dataframe['clean_tweet'] = new_tweet \n",
        "\n",
        "    def __convert_to_string(self):\n",
        "      self.new_tweet = []\n",
        "      for tweet_token_list in self.dataframe.clean_tweet:\n",
        "          tweet = ' '.join(tweet_token_list)\n",
        "          self.new_tweet.append(tweet)\n",
        "      self.dataframe['clean_tweet'] = self.new_tweet\n",
        "\n",
        "    def __remove_double_spacing(self):\n",
        "      new_tweet = []\n",
        "      for tweet in self.dataframe.clean_tweet:\n",
        "          tweet = re.sub(r'\\s+', ' ', tweet) \n",
        "          new_tweet.append(tweet)\n",
        "      self.dataframe['clean_tweet'] = new_tweet\n",
        "\n",
        "    def __remove_nan_in_clean_tweet(self):\n",
        "      self.dataframe['clean_tweet'].replace('', np.nan, inplace=True)\n",
        "      self.dataframe.dropna(subset=['clean_tweet'], inplace=True)\n",
        "      self.dataframe['clean_tweet'].str.replace(' ', '')\n",
        "      self.dataframe['clean_tweet'].str.strip()\n",
        "\n",
        "    def get_new_dataframe(self):\n",
        "      return self.dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hXyhWq-JrrbC"
      },
      "outputs": [],
      "source": [
        "tweet = Processing(df)\n",
        "df_new = tweet.get_new_dataframe()\n",
        "df_new.head(50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jFvPY6JAPIsc",
        "dramatic-fault",
        "bTIDDR1MQw2w",
        "LZlm0zMcV1zd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}