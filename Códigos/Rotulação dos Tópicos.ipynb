{"cells":[{"cell_type":"markdown","metadata":{"id":"9R2SrshvaWgK"},"source":["#Instalação "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1SI5-9LZl6K"},"outputs":[],"source":["# reading daaset from Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqwlQLzRvClB"},"outputs":[],"source":["!pip install wmd\n","!pip install fasttext\n","!pip install compress-fasttext"]},{"cell_type":"code","source":["!pip uninstall numpy\n","!pip install 'numpy>=1.18.0,<1.23.0'\n","!pip install compress-fasttext\n","import compress_fasttext"],"metadata":{"id":"BBxoB8vcIPdJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSeO0FuEaH2h"},"outputs":[],"source":[" # Packages to store and manipulate data\n","import pandas as pd\n","import numpy as np\n","# Example function using numpy:\n","from numpy import dot\n","from numpy.linalg import norm\n","import math\n","import os\n","import json\n","import csv\n","import string\n","import glob\n","import random\n","import time\n","import math\n","from datetime import datetime\n","pd.set_option('max_colwidth', 200)\n","\n","\n","# Plotting packages\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","import seaborn as sns \n","import pickle\n","import math\n","import wmd\n","from scipy import spatial\n","from scipy.spatial import distance\n","from scipy.spatial.distance import pdist, squareform\n","from collections import Counter\n","import plotly.express as px\n","%matplotlib inline\n","import plotly.express as px\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","\n","\n","# Package to clean text\n","import re\n","from keras.preprocessing.text import Tokenizer\n","import operator\n","from operator import itemgetter\n","from operator import itemgetter\n","from tqdm import tqdm\n","\n","\n","# Package NPL\n","import nltk\n","#nltk.download()\n","nltk.download('punkt') #This is a library that helps us tokenize words and phrases\n","nltk.download('stopwords')\n","nltk.download ( 'wordnet' ) #We use the data and methods in this library to stem our data\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from nltk.corpus import brown\n","stop_words = stopwords.words('english')\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk import pos_tag\n","from nltk import bigrams\n","from nltk.corpus import brown\n","from nltk.util import ngrams\n","from nltk import word_tokenize\n","from nltk import RegexpParser\n","\n","from textblob import TextBlob\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","from scipy.sparse import csr_matrix, issparse  # , todense\n","import sys\n","from collections import defaultdict\n","import multiprocessing\n","import fasttext\n","\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Sequential\n","from keras.layers import Input, Embedding, Dense, GRU, Dropout, Reshape, Bidirectional\n","from keras.callbacks import Callback, ModelCheckpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtE1sJx4E5HY"},"outputs":[],"source":["import compress_fasttext\n","small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load(\n","'https://github.com/avidale/compress-fasttext/releases/download/gensim-4-draft/ft_cc.en.300_freqprune_400K_100K_pq_300.bin')"]},{"cell_type":"markdown","metadata":{"id":"w-DXNJ5uZw7U"},"source":["#Tópicos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bawLECSdabLr"},"outputs":[],"source":["df_topic = pd.read_csv('/content/drive/bertopic-exp1_dia_utel.csv')\n","df_topic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEje7pnpdOHJ"},"outputs":[],"source":["list_topics = []\n","topic = []\n","for column in df_topic:\n","  topic = []\n","  for word in df_topic[str(column)]:\n","    topic.append(word)\n","  list_topics.append(topic)\n","\n","print(list_topics)"]},{"cell_type":"markdown","metadata":{"id":"O5c4YZ0IgDXB"},"source":["#Importar Títulos e Gerar Candidatos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmA8vpVNzMmu"},"outputs":[],"source":["list_topics\n","lst = []\n","for x in list_topics:\n","  for y in x:\n","    lst.append(y)\n","print(lst)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2UMCnlVpN7X"},"outputs":[],"source":["class Title:\n","\n","    def __init__(self, dataframe):\n","        self.dataframe = dataframe\n","        self.__lower_words()\n","        self.__remove_punctuation()\n","        self.__remove_double_spacing()\n","        self.__remove_numbers()\n","        self.__selecting_column()\n","        self.__tokenize_and_remove_stopwords_lemmatize()\n","        self.__convert_to_string()\n","    \n","    def __lower_words(self):\n","        self.dataframe['clean_title'] = self.dataframe['title'].apply(lambda x: x.lower())\n","        \n","    def __remove_punctuation(self):\n","        my_punctuation = '#!\"…¨$%&\\'”’(“)*+_,-./:;<=>?[\\\\]^_`{|}~•@'\n","        new_tweet = []\n","        for tweet in self.dataframe.clean_title:\n","            tweet = re.sub('['+my_punctuation + ']+', ' ', tweet)\n","            new_tweet.append(tweet)\n","        self.dataframe['clean_title'] = new_tweet\n","        \n","    def __remove_double_spacing(self):\n","        new_tweet = []\n","        for tweet in self.dataframe.clean_title:\n","            tweet = re.sub(r'\\s+', ' ', tweet) \n","            new_tweet.append(tweet)\n","        self.dataframe['clean_title'] = new_tweet\n","        \n","    def __remove_numbers(self):\n","        new_tweet = []\n","        for tweet in self.dataframe.clean_title:\n","            tweet = re.sub('([0-9]+)', '', tweet)\n","            tweet = tweet.strip()\n","            new_tweet.append(tweet)\n","        self.dataframe['clean_title'] = new_tweet\n","\n","    def __selecting_column(self):\n","        self.dataframe.dropna(subset=['clean_title'], axis=0, inplace = True)\n","        self.clean_title = self.dataframe['clean_title']\n","        self.text = \" \".join(s for s in self.clean_title).lower()\n","        \n","    def __tokenize_and_remove_stopwords_lemmatize(self,bigrams=False):\n","        lemmatiser = WordNetLemmatizer()\n","        self.stop_words = stopwords.words('english')\n","        self.stop_words.extend(lst) \n","        self.stop_words.extend(['list', 'redirect', 'category', 'section', 'from'])\n","\n","        new_tweet = []\n","        for tweet in self.dataframe.clean_title:\n","            tweet_token = [word for word in tweet.split(' ') if len(word)> 2] # Removing short words - less than three\n","            tweet_token_list = [word for word in tweet_token if word not in self.stop_words]\n","            new_tweet.append(tweet_token_list)\n","        self.dataframe['clean_title_tokenize'] = new_tweet\n","    \n","    def __convert_to_string(self):\n","        self.new_tweet = []\n","        for tweet_token_list in self.dataframe.clean_title_tokenize:\n","            tweet = ' '.join(tweet_token_list)\n","            self.new_tweet.append(tweet)\n","        self.dataframe['clean_title'] = self.new_tweet\n","\n","    def get_tokens_set(self):\n","      tokens = []\n","      for tweet in self.dataframe.clean_title:\n","        tokens_set = tweet.split()\n","        for token in tokens_set:\n","          tokens.append(token)\n","      return tokens\n","      \n","    def get_new_dataframe(self):\n","        return self.dataframe\n","        \n","df_label_title= pd.read_csv(\"/content/drive/Bertopic exp1.train.csv\", index_col=False, dtype='unicode')\n","\n","\n","label_topic = Title(df_label_title)\n","df_label = label_topic.get_new_dataframe()\n","tokens = label_topic.get_tokens_set()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvysBZgxCitT"},"outputs":[],"source":["# Count the total number of occurences of each word\n","word_counts = {}\n","for title in df_label.clean_title:\n","    for word in title.split():\n","        if word not in word_counts:\n","            word_counts[word] = 1\n","        else:\n","            word_counts[word] += 1\n","print(\"Size of Vocabulary:\", len(word_counts.keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOyPVGdQEcU0"},"outputs":[],"source":["word_counts = {k: v for k, v in sorted(word_counts.items(),reverse=True, key=lambda item: item[1])}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIp7PGnyFQOP"},"outputs":[],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df_label.clean_title)\n","word_index = tokenizer.word_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9SVg2-IX5TC"},"outputs":[],"source":["new_tokens = []\n","for t in tokens:\n","  types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n","  tag = nltk.pos_tag([t])\n","  if tag[0][1] in types:\n","    new_tokens.append(t)\n","    \n","len(new_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVnReIe9uTUJ"},"outputs":[],"source":["import nltk\n","bigrams = nltk.collocations.BigramAssocMeasures()\n","trigrams = nltk.collocations.TrigramAssocMeasures()\n","bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(new_tokens)\n","trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(new_tokens)\n","\n","#bigrams\n","bigram_freq = bigramFinder.ngram_fd.items()\n","bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n","#trigrams\n","trigram_freq = trigramFinder.ngram_fd.items()\n","trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n","\n","#get english stopwords\n","stopwords_en = set(stopwords.words('english'))\n","\n","        #JJ  - adjective ‘cheap’ \n","        #JJR  - adjective, comparative ‘cheaper’ \n","        #JJS  - adjective, superlative ‘cheapest’\n","        #NN   - noun, singular ‘table’ \n","        #NNS  - noun plural ‘undergraduates’ \n","        #NNP  - proper noun, singular ‘Rohan' \n","        #NNPS - proper noun, plural ‘Indians’ \n","\n","        #Bigrams: (Noun, Noun), (Adjective, Noun)\n","        #Trigrams: (Adjective/Noun, Anything, Adjective/Noun)\n","\n","#function to filter for ADJ/NN bigrams\n","def rightTypes(ngram):\n","    if '-pron-' in ngram or 't' in ngram:\n","        return False\n","    for word in ngram:\n","        if word in stopwords_en or word.isspace():\n","            return False\n","    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n","    second_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n","    tags = nltk.pos_tag(ngram)\n","    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n","        return True\n","    else:\n","        return False\n","\n","#function to filter for trigrams\n","def rightTypesTri(ngram):\n","    if '-pron-' in ngram or 't' in ngram:\n","        return False\n","    for word in ngram:\n","        if word in stopwords_en or word.isspace():\n","            return False\n","    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n","    second_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS') ########\n","    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n","    tags = nltk.pos_tag(ngram)\n","    if tags[0][1] in first_type and tags[1][1] in second_type and tags[2][1] in third_type:\n","        return True\n","    else:\n","        return False\n","\n","#filter bigrams\n","filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda new_tokens: rightTypes(new_tokens))]\n","#filter trigrams\n","filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda new_tokens: rightTypesTri(new_tokens))]\n","\n","\n","#filter for only those with more than 2 occurences\n","bigramPMITable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.pmi)), columns=['candidate','PMI']).sort_values(by='PMI', ascending=False)\n","trigramPMITable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.pmi)), columns=['candidate','PMI']).sort_values(by='PMI', ascending=False)\n","\n","candidates = pd.concat([bigramPMITable, trigramPMITable]).sort_values(by='PMI', ascending=False)\n","print(len(candidates)) ##\n","\n","candidates = candidates[(candidates['PMI']>4)]\n","\n","list_candidate = []\n","string_candidate = []\n","for candidate in candidates.candidate:\n","  list_unit = []\n","  ngram = \" \".join(candidate)\n","  list_unit.append(ngram)\n","  list_candidate.append(list_unit)\n","  string_candidate.append(ngram)\n","\n","candidates[\"candidates_string\"] = string_candidate\n","candidates[\"candidates_list\"] = list_candidate\n","candidates.reset_index(drop=True, inplace=True)\n","candidates "]},{"cell_type":"markdown","metadata":{"id":"xdvzJeSQgMTv"},"source":["#Rótulo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHPCaFALM5QW"},"outputs":[],"source":["def calculate_cosine_distance(a, b):\n","    cosine_distance = float(spatial.distance.cosine(a, b))\n","    return cosine_distance\n","\n","def calculate_cosine_similarity(a, b):\n","    cosine_similarity = 1 - calculate_cosine_distance(a, b)\n","    return cosine_similarity\n","\n","def calculate_angular_distance(a, b):\n","    cosine_similarity = calculate_cosine_similarity(a, b)\n","    angular_distance = math.acos(cosine_similarity) / math.pi\n","    return angular_distance\n","\n","def calculate_angular_similarity(a, b):\n","    angular_similarity = 1 - calculate_angular_distance(a, b)\n","    return angular_similarity\n","\n","def distance(v1, v2):\n","    return np.sqrt(np.sum((v1 - v2) ** 2))  \n","\n","def minmax_norm(df_input):\n","    return (df_input - df_input.min()) / ( df_input.max() - df_input.min()) \n","\n","def word_mover_similarity(a, b):\n","    # your distance score needs to be converted to a similarity score\n","    similarity = TODO_IMPLEMENT(a, b)\n","    return similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X29_-0fRTTjg"},"outputs":[],"source":["name = ''\n","dic_cand = {} #soma das distancias de cada palavra para calcular com os tokens\n","# (x + y + z)/3 dist a\n","\n","for n in range(0, 10):\n","  name = 'Topic '+ str(n)\n","  dic_cand[name] = {}\n","  dic_cand[name]['cosine'] = {}\n","  med_cos = {}\n","\n","  for c in candidates.candidate:\n","    test = 0\n","    cal_cos = []\n","    for i in range(len(c)):\n","      primeiro = small_model[c[i]]\n","      test += primeiro\n","    media = test/len(c)\n","\n","    for t in df_topic[name]:\n","      array_t = small_model[t]\n","      cos_sim = calculate_cosine_similarity(media, array_t)\n","      cal_cos.append(cos_sim)\n","\n","    sum_cos = 0\n","    for i in cal_cos:\n","      sum_cos += i\n","    med_cos[sum_cos/10] = c\n","    dic_cand[name]['cosine'] = med_cos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CghBLlQlVCsw"},"outputs":[],"source":["name = ''\n","dic_cand_filter = {}\n","\n","for n in range(0, 10):\n","  name = 'Topic '+ str(n)\n","  dic_cand_filter[name] = {}\n","  a = sorted(dic_cand[name]['cosine'].items(), reverse=True) #classificando os com valores maiores\n","  edit_a = a[:200]\n","  lista_name = []\n","\n","  for i in edit_a:\n","    lista_name.append(i[1])\n","  dic_cand_filter[name] = lista_name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGGzQ6xlVyud"},"outputs":[],"source":["name = ''\n","dic_cand_best = {}\n","\n","for n in range(10):\n","  name = 'Topic '+ str(n)\n","  dic_cand_best[name] = {}\n","  dic_cand_best[name]['cosine'] = {} \n","  dic_cand_best[name]['angular'] = {}\n","  dic_cand_best[name]['euclidean'] = {}\n","\n","  med_cos = {}\n","  med_ang = {}\n","  med_euclidean = {}\n","\n","  for c in dic_cand_filter[name]:\n","    test = 0\n","    for i in range(len(c)):\n","      primeiro = small_model[c[i]]\n","      test += primeiro\n","    media = test/len(c)\n","    str_c = ' '.join(c)\n","\n","    cal_cos = []\n","    cal_ang = []\n","    cal_euclidean = []\n","\n","    for t in df_topic[name]:\n","      array_t = small_model[t]\n","\n","      cos_sim = calculate_cosine_similarity(media, array_t)\n","      ang_sim = calculate_angular_similarity(array_t, media)\n","      euclidean = np.linalg.norm(array_t - media)\n","\n","      cal_cos.append(cos_sim)\n","      cal_ang.append(ang_sim)\n","      cal_euclidean.append(euclidean)\n","\n","    sum_cos = 0\n","    sum_jacc = 0\n","    sum_ang = 0\n","    sum_euclidean = 0\n","\n","    for i in cal_cos:\n","      sum_cos += i\n","    for j in cal_ang:\n","      sum_ang += j\n","    for w in cal_euclidean:\n","      sum_euclidean += w\n","\n","    med_cos[str_c] = sum_cos/10\n","    med_ang[str_c] = sum_ang/10\n","    med_euclidean[str_c] = sum_euclidean/10\n","\n","    dic_cand_best[name]['cosine'] = med_cos\n","    dic_cand_best[name]['cosine2'] = med_cos\n","    dic_cand_best[name]['angular'] = med_ang\n","    dic_cand_best[name]['euclidean'] = med_euclidean\n","\n","  df_cand_select = pd.DataFrame(data=dic_cand_best[name])\n","  df_cand_select['value'] = df_cand_select.sum(axis=1)/len(dic_cand_best[name].keys())\n","  df_cand_select = df_cand_select.reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AwvcOrNnFdHC"},"outputs":[],"source":["dic_cand_final = pd.DataFrame()\n","\n","topic_num = []\n","topic_word = []\n","topic_label = []\n","\n","for n in range(0, 10):\n","  name = 'Topic '+ str(n)\n","  topic_num.append(name)\n","\n","\n","  df_cand_select = pd.DataFrame(data=dic_cand_best[name])\n","  df_cand_select['cosine2'] = df_cand_select['cosine']\n","  df_cand_select = minmax_norm(df_cand_select)\n","  df_cand_select['value'] = df_cand_select.sum(axis=1)/len(dic_cand_best[name].keys())\n","  df_cand_select = df_cand_select.reset_index()\n","\n","  best_med = 0\n","  cand_name = ''\n","  for i in range(len(df_cand_select['value'])):\n","      if df_cand_select['value'][i] > best_med:\n","        best_med = df_cand_select['value'][i]\n","        cand_name = df_cand_select['index'][i]\n","\n","  tokens_lda = []\n","  for t in df_topic[name]:\n","    tokens_lda.append(t)\n","\n","  topic_word.append(tokens_lda)\n","\n","  tokens_lda_str = \", \".join(tokens_lda)\n","  print(str(name) +\": \"+str(tokens_lda_str))\n","\n","  df_cand_select  = df_cand_select.sort_values(by='value', ascending=False).head(5)\n","  l = []\n","  for i in df_cand_select[:1]['index']:\n","    l.append(i)\n","    topic_label.append(i)\n","  l_str = \", \".join(l)\n","  print(\"candidates: \" +str(l_str))\n","  print(\"\\n\")\n","\n","dic_cand_final['Indice'] = topic_num\n","dic_cand_final['Tópico'] = topic_word\n","dic_cand_final['Rótulo'] = topic_label"]},{"cell_type":"code","source":["dic_cand_final"],"metadata":{"id":"U7iLrf-CnUJI"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1HBuXX51dYk2ChearHleqzXIP_DvdyUFr","timestamp":1666709072434},{"file_id":"1iNXR5TLJOLnV3MwDfY-8EcdIqYX5I-26","timestamp":1666701372621},{"file_id":"1TWQVpeg7X68zgwWxhuk3j4ZHRSqdX1fV","timestamp":1666700553478},{"file_id":"156BIQbQeYzubqL21SPp1nVIeWfJH0SOE","timestamp":1651556580956}],"collapsed_sections":["9R2SrshvaWgK","w-DXNJ5uZw7U","O5c4YZ0IgDXB","xdvzJeSQgMTv"],"authorship_tag":"ABX9TyNmAR6N1IQ2ysFs7s2/hzrp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}