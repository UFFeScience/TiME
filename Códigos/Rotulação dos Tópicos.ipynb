{"cells":[{"cell_type":"markdown","metadata":{"id":"9R2SrshvaWgK"},"source":["#Instalação "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1SI5-9LZl6K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675446944674,"user_tz":180,"elapsed":152115,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"}},"outputId":"d0ed57f6-daa4-430e-913d-e475f337a304"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# reading daaset from Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":80747,"status":"ok","timestamp":1675447025409,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"},"user_tz":180},"id":"MqwlQLzRvClB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f9e605b-8afa-494d-cd51-903139713a25"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wmd\n","  Downloading wmd-1.3.2.tar.gz (104 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from wmd) (1.21.6)\n","Building wheels for collected packages: wmd\n","  Building wheel for wmd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wmd: filename=wmd-1.3.2-cp38-cp38-linux_x86_64.whl size=1257985 sha256=a06c1545bab1fbd6fc0ce326dd09637202eaacf46f6c62e21a5f420c0f8982ff\n","  Stored in directory: /root/.cache/pip/wheels/eb/4c/cd/40ec1e13bfd149162c9a69f5b07728410ea9af264e66cea28d\n","Successfully built wmd\n","Installing collected packages: wmd\n","Successfully installed wmd-1.3.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fasttext\n","  Downloading fasttext-0.9.2.tar.gz (68 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pybind11>=2.2\n","  Using cached pybind11-2.10.3-py3-none-any.whl (222 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=4402294 sha256=9dad3569e0898c548f2becb46d1b5472c3a74d12fa190a48c3d382658db4d8b2\n","  Stored in directory: /root/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.2 pybind11-2.10.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting compress-fasttext\n","  Downloading compress-fasttext-0.1.3.tar.gz (14 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gensim>=4.0.0\n","  Downloading gensim-4.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from compress-fasttext) (1.21.6)\n","Collecting FuzzyTM>=0.4.0\n","  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from gensim>=4.0.0->compress-fasttext) (1.7.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=4.0.0->compress-fasttext) (6.3.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (1.3.5)\n","Collecting pyfume\n","  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (2022.7.1)\n","Collecting fst-pso\n","  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting simpful\n","  Downloading simpful-2.9.0-py3-none-any.whl (30 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (1.15.0)\n","Collecting miniful\n","  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (2.25.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (4.0.0)\n","Building wheels for collected packages: compress-fasttext, fst-pso, miniful\n","  Building wheel for compress-fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for compress-fasttext: filename=compress_fasttext-0.1.3-py3-none-any.whl size=14601 sha256=3f3411088670691bce9945c33b8e6367d7eee30b0f7b5a85c6aedcc9013b6383\n","  Stored in directory: /root/.cache/pip/wheels/c7/63/9f/39db0410175167cee5eeae4fde2405d957cd05c1d8811a51cf\n","  Building wheel for fst-pso (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20443 sha256=18d1c00e482fe17d0099606d673361512819f36acc63f0c2341f2d2a3cd7d4e4\n","  Stored in directory: /root/.cache/pip/wheels/6a/65/c4/d27eeee9ba3fc150a0dae150519591103b9e0dbffde3ae77dc\n","  Building wheel for miniful (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3530 sha256=438578e799d4129bf1c79cfa890c6d998f73ac06e15af8fec49e69d454ece49e\n","  Stored in directory: /root/.cache/pip/wheels/ba/d9/a0/ddd93af16d5855dd9bad417623e70948fdac119d1d34fb17c8\n","Successfully built compress-fasttext fst-pso miniful\n","Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM, gensim, compress-fasttext\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 3.6.0\n","    Uninstalling gensim-3.6.0:\n","      Successfully uninstalled gensim-3.6.0\n","Successfully installed FuzzyTM-2.0.5 compress-fasttext-0.1.3 fst-pso-1.8.1 gensim-4.3.0 miniful-0.0.6 pyfume-0.2.25 simpful-2.9.0\n"]}],"source":["!pip install wmd\n","!pip install fasttext\n","!pip install compress-fasttext"]},{"cell_type":"code","source":["!pip uninstall numpy\n","!pip install 'numpy>=1.18.0,<1.23.0'\n","!pip install compress-fasttext\n","import compress_fasttext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":783},"id":"BBxoB8vcIPdJ","executionInfo":{"status":"ok","timestamp":1675447112526,"user_tz":180,"elapsed":35704,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"}},"outputId":"431ada88-0059-427c-a655-78e724121f5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 1.22.4\n","Uninstalling numpy-1.22.4:\n","  Would remove:\n","    /usr/local/bin/f2py\n","    /usr/local/bin/f2py3\n","    /usr/local/bin/f2py3.8\n","    /usr/local/lib/python3.8/dist-packages/numpy-1.22.4.dist-info/*\n","    /usr/local/lib/python3.8/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n","    /usr/local/lib/python3.8/dist-packages/numpy.libs/libopenblas64_p-r0-2f7c42d4.3.18.so\n","    /usr/local/lib/python3.8/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n","    /usr/local/lib/python3.8/dist-packages/numpy/*\n","Proceed (Y/n)? y\n","  Successfully uninstalled numpy-1.22.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting numpy<1.23.0,>=1.18.0\n","  Using cached numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n","Installing collected packages: numpy\n","Successfully installed numpy-1.22.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: compress-fasttext in /usr/local/lib/python3.8/dist-packages (0.1.3)\n","Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from compress-fasttext) (4.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from compress-fasttext) (1.22.4)\n","Requirement already satisfied: FuzzyTM>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from gensim>=4.0.0->compress-fasttext) (2.0.5)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from gensim>=4.0.0->compress-fasttext) (1.7.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=4.0.0->compress-fasttext) (6.3.0)\n","Requirement already satisfied: pyfume in /usr/local/lib/python3.8/dist-packages (from FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (0.2.25)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (2022.7.1)\n","Requirement already satisfied: simpful in /usr/local/lib/python3.8/dist-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (2.9.0)\n","Requirement already satisfied: fst-pso in /usr/local/lib/python3.8/dist-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (1.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (1.15.0)\n","Requirement already satisfied: miniful in /usr/local/lib/python3.8/dist-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (0.0.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (2.25.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim>=4.0.0->compress-fasttext) (1.24.3)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12072,"status":"ok","timestamp":1675447125678,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"},"user_tz":180},"id":"fSeO0FuEaH2h","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7cbc46ce-e856-47d9-a8ba-3d9c5a4538bd"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n"]}],"source":[" # Packages to store and manipulate data\n","import pandas as pd\n","import numpy as np\n","# Example function using numpy:\n","from numpy import dot\n","from numpy.linalg import norm\n","import math\n","import os\n","import json\n","import csv\n","import string\n","import glob\n","import random\n","import time\n","import math\n","from datetime import datetime\n","pd.set_option('max_colwidth', 200)\n","\n","\n","# Plotting packages\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","import seaborn as sns \n","import pickle\n","import math\n","import wmd\n","from scipy import spatial\n","from scipy.spatial import distance\n","from scipy.spatial.distance import pdist, squareform\n","from collections import Counter\n","import plotly.express as px\n","%matplotlib inline\n","import plotly.express as px\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","\n","\n","# Package to clean text\n","import re\n","from keras.preprocessing.text import Tokenizer\n","import operator\n","from operator import itemgetter\n","from operator import itemgetter\n","from tqdm import tqdm\n","\n","\n","# Package NPL\n","import nltk\n","#nltk.download()\n","nltk.download('punkt') #This is a library that helps us tokenize words and phrases\n","nltk.download('stopwords')\n","nltk.download ( 'wordnet' ) #We use the data and methods in this library to stem our data\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from nltk.corpus import brown\n","stop_words = stopwords.words('english')\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk import pos_tag\n","from nltk import bigrams\n","from nltk.corpus import brown\n","from nltk.util import ngrams\n","from nltk import word_tokenize\n","from nltk import RegexpParser\n","\n","from textblob import TextBlob\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","from scipy.sparse import csr_matrix, issparse  # , todense\n","import sys\n","from collections import defaultdict\n","import multiprocessing\n","import fasttext\n","\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Sequential\n","from keras.layers import Input, Embedding, Dense, GRU, Dropout, Reshape, Bidirectional\n","from keras.callbacks import Callback, ModelCheckpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtE1sJx4E5HY"},"outputs":[],"source":["import compress_fasttext\n","small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load(\n","'https://github.com/avidale/compress-fasttext/releases/download/gensim-4-draft/ft_cc.en.300_freqprune_400K_100K_pq_300.bin')"]},{"cell_type":"markdown","metadata":{"id":"w-DXNJ5uZw7U"},"source":["#Tópicos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bawLECSdabLr","colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"status":"ok","timestamp":1675448268245,"user_tz":180,"elapsed":265,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"}},"outputId":"48f6d970-34bf-48af-91c8-df1123e992c6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["    Topic 0  Topic 1       Topic 2   Topic 3    Topic 4    Topic 5  \\\n","0   reports   wordle    basketball      inch      album  poisoning   \n","1      snow     word          boys   reports      kanye       food   \n","2      inch    guess         girls  freezing       song     cheese   \n","3       mph   streak          game      snow   released      salad   \n","4      wind      nyt         state      rain       drop   sandwich   \n","5  cocorahs     phew        soccer       cst      songs    chicken   \n","6   station  nytimes          team     radio     listen   diarrhea   \n","7     today  guesses         final   amateur  listening        ate   \n","8       cst     play  championship     sleet       good   vomiting   \n","9     biden    close    tournament    public       best       sick   \n","\n","       Topic 6   Topic 7   Topic 8  Topic 9  \n","0         vote    artist  cocorahs   church  \n","1       voting       art   station  worship  \n","2        putin    tattoo       mst   pastor  \n","3        trump       nft      inch     join  \n","4  republicans  painting   reports    bible  \n","5          gop      work      snow    study  \n","6   republican   artists   boulder   prayer  \n","7       russia    design   springs   sunday  \n","8        party     piece  montrose      god  \n","9     praising   project       ese  baptist  "],"text/html":["\n","  <div id=\"df-717dc460-956e-4d18-ae9a-cb962f2e84a6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Topic 0</th>\n","      <th>Topic 1</th>\n","      <th>Topic 2</th>\n","      <th>Topic 3</th>\n","      <th>Topic 4</th>\n","      <th>Topic 5</th>\n","      <th>Topic 6</th>\n","      <th>Topic 7</th>\n","      <th>Topic 8</th>\n","      <th>Topic 9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>reports</td>\n","      <td>wordle</td>\n","      <td>basketball</td>\n","      <td>inch</td>\n","      <td>album</td>\n","      <td>poisoning</td>\n","      <td>vote</td>\n","      <td>artist</td>\n","      <td>cocorahs</td>\n","      <td>church</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>snow</td>\n","      <td>word</td>\n","      <td>boys</td>\n","      <td>reports</td>\n","      <td>kanye</td>\n","      <td>food</td>\n","      <td>voting</td>\n","      <td>art</td>\n","      <td>station</td>\n","      <td>worship</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>inch</td>\n","      <td>guess</td>\n","      <td>girls</td>\n","      <td>freezing</td>\n","      <td>song</td>\n","      <td>cheese</td>\n","      <td>putin</td>\n","      <td>tattoo</td>\n","      <td>mst</td>\n","      <td>pastor</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>mph</td>\n","      <td>streak</td>\n","      <td>game</td>\n","      <td>snow</td>\n","      <td>released</td>\n","      <td>salad</td>\n","      <td>trump</td>\n","      <td>nft</td>\n","      <td>inch</td>\n","      <td>join</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>wind</td>\n","      <td>nyt</td>\n","      <td>state</td>\n","      <td>rain</td>\n","      <td>drop</td>\n","      <td>sandwich</td>\n","      <td>republicans</td>\n","      <td>painting</td>\n","      <td>reports</td>\n","      <td>bible</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>cocorahs</td>\n","      <td>phew</td>\n","      <td>soccer</td>\n","      <td>cst</td>\n","      <td>songs</td>\n","      <td>chicken</td>\n","      <td>gop</td>\n","      <td>work</td>\n","      <td>snow</td>\n","      <td>study</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>station</td>\n","      <td>nytimes</td>\n","      <td>team</td>\n","      <td>radio</td>\n","      <td>listen</td>\n","      <td>diarrhea</td>\n","      <td>republican</td>\n","      <td>artists</td>\n","      <td>boulder</td>\n","      <td>prayer</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>today</td>\n","      <td>guesses</td>\n","      <td>final</td>\n","      <td>amateur</td>\n","      <td>listening</td>\n","      <td>ate</td>\n","      <td>russia</td>\n","      <td>design</td>\n","      <td>springs</td>\n","      <td>sunday</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>cst</td>\n","      <td>play</td>\n","      <td>championship</td>\n","      <td>sleet</td>\n","      <td>good</td>\n","      <td>vomiting</td>\n","      <td>party</td>\n","      <td>piece</td>\n","      <td>montrose</td>\n","      <td>god</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>biden</td>\n","      <td>close</td>\n","      <td>tournament</td>\n","      <td>public</td>\n","      <td>best</td>\n","      <td>sick</td>\n","      <td>praising</td>\n","      <td>project</td>\n","      <td>ese</td>\n","      <td>baptist</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-717dc460-956e-4d18-ae9a-cb962f2e84a6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-717dc460-956e-4d18-ae9a-cb962f2e84a6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-717dc460-956e-4d18-ae9a-cb962f2e84a6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":185}],"source":["df_topic = pd.read_csv('/content/drive/bertopic-exp1_dia_utel.csv')\n","df_topic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEje7pnpdOHJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675448268785,"user_tz":180,"elapsed":15,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"}},"outputId":"4fb123fb-8776-4ff9-d256-291075314e6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['reports', 'snow', 'inch', 'mph', 'wind', 'cocorahs', 'station', 'today', 'cst', 'biden'], ['wordle', 'word', 'guess', 'streak', 'nyt', 'phew', 'nytimes', 'guesses', 'play', 'close'], ['basketball', 'boys', 'girls', 'game', 'state', 'soccer', 'team', 'final', 'championship', 'tournament'], ['inch', 'reports', 'freezing', 'snow', 'rain', 'cst', 'radio', 'amateur', 'sleet', 'public'], ['album', 'kanye', 'song', 'released', 'drop', 'songs', 'listen', 'listening', 'good', 'best'], ['poisoning', 'food', 'cheese', 'salad', 'sandwich', 'chicken', 'diarrhea', 'ate', 'vomiting', 'sick'], ['vote', 'voting', 'putin', 'trump', 'republicans', 'gop', 'republican', 'russia', 'party', 'praising'], ['artist', 'art', 'tattoo', 'nft', 'painting', 'work', 'artists', 'design', 'piece', 'project'], ['cocorahs', 'station', 'mst', 'inch', 'reports', 'snow', 'boulder', 'springs', 'montrose', 'ese'], ['church', 'worship', 'pastor', 'join', 'bible', 'study', 'prayer', 'sunday', 'god', 'baptist']]\n"]}],"source":["list_topics = []\n","topic = []\n","for column in df_topic:\n","  topic = []\n","  for word in df_topic[str(column)]:\n","    topic.append(word)\n","  list_topics.append(topic)\n","\n","print(list_topics)"]},{"cell_type":"markdown","metadata":{"id":"O5c4YZ0IgDXB"},"source":["#Importar Títulos e Gerar Candidatos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmA8vpVNzMmu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675448268786,"user_tz":180,"elapsed":12,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"}},"outputId":"696ed7f7-f5f8-4bb4-a046-acd4951de111"},"outputs":[{"output_type":"stream","name":"stdout","text":["['reports', 'snow', 'inch', 'mph', 'wind', 'cocorahs', 'station', 'today', 'cst', 'biden', 'wordle', 'word', 'guess', 'streak', 'nyt', 'phew', 'nytimes', 'guesses', 'play', 'close', 'basketball', 'boys', 'girls', 'game', 'state', 'soccer', 'team', 'final', 'championship', 'tournament', 'inch', 'reports', 'freezing', 'snow', 'rain', 'cst', 'radio', 'amateur', 'sleet', 'public', 'album', 'kanye', 'song', 'released', 'drop', 'songs', 'listen', 'listening', 'good', 'best', 'poisoning', 'food', 'cheese', 'salad', 'sandwich', 'chicken', 'diarrhea', 'ate', 'vomiting', 'sick', 'vote', 'voting', 'putin', 'trump', 'republicans', 'gop', 'republican', 'russia', 'party', 'praising', 'artist', 'art', 'tattoo', 'nft', 'painting', 'work', 'artists', 'design', 'piece', 'project', 'cocorahs', 'station', 'mst', 'inch', 'reports', 'snow', 'boulder', 'springs', 'montrose', 'ese', 'church', 'worship', 'pastor', 'join', 'bible', 'study', 'prayer', 'sunday', 'god', 'baptist']\n"]}],"source":["list_topics\n","lst = []\n","for x in list_topics:\n","  for y in x:\n","    lst.append(y)\n","print(lst)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2UMCnlVpN7X"},"outputs":[],"source":["class Title:\n","\n","    def __init__(self, dataframe):\n","        self.dataframe = dataframe\n","        self.__lower_words()\n","        self.__remove_punctuation()\n","        self.__remove_double_spacing()\n","        self.__remove_numbers()\n","        self.__selecting_column()\n","        self.__tokenize_and_remove_stopwords_lemmatize()\n","        self.__convert_to_string()\n","    \n","    def __lower_words(self):\n","        self.dataframe['clean_title'] = self.dataframe['title'].apply(lambda x: x.lower())\n","        \n","    def __remove_punctuation(self):\n","        my_punctuation = '#!\"…¨$%&\\'”’(“)*+_,-./:;<=>?[\\\\]^_`{|}~•@'\n","        new_tweet = []\n","        for tweet in self.dataframe.clean_title:\n","            tweet = re.sub('['+my_punctuation + ']+', ' ', tweet)\n","            new_tweet.append(tweet)\n","        self.dataframe['clean_title'] = new_tweet\n","        \n","    def __remove_double_spacing(self):\n","        new_tweet = []\n","        for tweet in self.dataframe.clean_title:\n","            tweet = re.sub(r'\\s+', ' ', tweet) \n","            new_tweet.append(tweet)\n","        self.dataframe['clean_title'] = new_tweet\n","        \n","    def __remove_numbers(self):\n","        new_tweet = []\n","        for tweet in self.dataframe.clean_title:\n","            tweet = re.sub('([0-9]+)', '', tweet)\n","            tweet = tweet.strip()\n","            new_tweet.append(tweet)\n","        self.dataframe['clean_title'] = new_tweet\n","\n","    def __selecting_column(self):\n","        self.dataframe.dropna(subset=['clean_title'], axis=0, inplace = True)\n","        self.clean_title = self.dataframe['clean_title']\n","        self.text = \" \".join(s for s in self.clean_title).lower()\n","        \n","    def __tokenize_and_remove_stopwords_lemmatize(self,bigrams=False):\n","        lemmatiser = WordNetLemmatizer()\n","        self.stop_words = stopwords.words('english')\n","        self.stop_words.extend(lst) \n","        self.stop_words.extend(['list', 'redirect', 'category', 'section', 'from'])\n","\n","        new_tweet = []\n","        for tweet in self.dataframe.clean_title:\n","            tweet_token = [word for word in tweet.split(' ') if len(word)> 2] # Removing short words - less than three\n","            tweet_token_list = [word for word in tweet_token if word not in self.stop_words]\n","            new_tweet.append(tweet_token_list)\n","        self.dataframe['clean_title_tokenize'] = new_tweet\n","    \n","    def __convert_to_string(self):\n","        self.new_tweet = []\n","        for tweet_token_list in self.dataframe.clean_title_tokenize:\n","            tweet = ' '.join(tweet_token_list)\n","            self.new_tweet.append(tweet)\n","        self.dataframe['clean_title'] = self.new_tweet\n","\n","    def get_tokens_set(self):\n","      tokens = []\n","      for tweet in self.dataframe.clean_title:\n","        tokens_set = tweet.split()\n","        for token in tokens_set:\n","          tokens.append(token)\n","      return tokens\n","      \n","    def get_new_dataframe(self):\n","        return self.dataframe\n","        \n","df_label_title= pd.read_csv(\"/content/drive/Bertopic exp1.train.csv\", index_col=False, dtype='unicode')\n","\n","\n","label_topic = Title(df_label_title)\n","df_label = label_topic.get_new_dataframe()\n","tokens = label_topic.get_tokens_set()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvysBZgxCitT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675448269035,"user_tz":180,"elapsed":10,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"}},"outputId":"fd74f09b-30fd-428b-940c-a7a6a3f7b1ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Size of Vocabulary: 269\n"]}],"source":["# Count the total number of occurences of each word\n","word_counts = {}\n","for title in df_label.clean_title:\n","    for word in title.split():\n","        if word not in word_counts:\n","            word_counts[word] = 1\n","        else:\n","            word_counts[word] += 1\n","print(\"Size of Vocabulary:\", len(word_counts.keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOyPVGdQEcU0"},"outputs":[],"source":["word_counts = {k: v for k, v in sorted(word_counts.items(),reverse=True, key=lambda item: item[1])}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIp7PGnyFQOP"},"outputs":[],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df_label.clean_title)\n","word_index = tokenizer.word_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9SVg2-IX5TC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675448269036,"user_tz":180,"elapsed":9,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"}},"outputId":"746c807a-eb74-4aec-8477-0b09ab97adc8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["344"]},"metadata":{},"execution_count":193}],"source":["new_tokens = []\n","for t in tokens:\n","  types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n","  tag = nltk.pos_tag([t])\n","  if tag[0][1] in types:\n","    new_tokens.append(t)\n","    \n","len(new_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVnReIe9uTUJ","colab":{"base_uri":"https://localhost:8080/","height":441},"executionInfo":{"status":"ok","timestamp":1675448269319,"user_tz":180,"elapsed":290,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"}},"outputId":"e4639260-8287-47b4-8ccd-e74f28be5007"},"outputs":[{"output_type":"stream","name":"stdout","text":["653\n"]},{"output_type":"execute_result","data":{"text/plain":["                           candidate        PMI            candidates_string  \\\n","0                 (new, york, times)  16.852530               new york times   \n","1    (championships, ncaa, division)  16.852530  championships ncaa division   \n","2    (artificial, intelligence, new)  16.852530  artificial intelligence new   \n","3            (banana, bacon, burger)  16.852530          banana bacon burger   \n","4             (band, studio, albums)  16.852530           band studio albums   \n","..                               ...        ...                          ...   \n","643           (weather, terminology)   5.104337          weather terminology   \n","644           (states, presidential)   4.841302          states presidential   \n","645                  (winter, storm)   4.841302                 winter storm   \n","646                (severe, weather)   4.519374               severe weather   \n","647                (climate, united)   4.033947               climate united   \n","\n","                   candidates_list  \n","0                 [new york times]  \n","1    [championships ncaa division]  \n","2    [artificial intelligence new]  \n","3            [banana bacon burger]  \n","4             [band studio albums]  \n","..                             ...  \n","643          [weather terminology]  \n","644          [states presidential]  \n","645                 [winter storm]  \n","646               [severe weather]  \n","647               [climate united]  \n","\n","[648 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-7a891744-ad0d-4184-a0d6-cd81312d3f2c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>candidate</th>\n","      <th>PMI</th>\n","      <th>candidates_string</th>\n","      <th>candidates_list</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(new, york, times)</td>\n","      <td>16.852530</td>\n","      <td>new york times</td>\n","      <td>[new york times]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(championships, ncaa, division)</td>\n","      <td>16.852530</td>\n","      <td>championships ncaa division</td>\n","      <td>[championships ncaa division]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(artificial, intelligence, new)</td>\n","      <td>16.852530</td>\n","      <td>artificial intelligence new</td>\n","      <td>[artificial intelligence new]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(banana, bacon, burger)</td>\n","      <td>16.852530</td>\n","      <td>banana bacon burger</td>\n","      <td>[banana bacon burger]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(band, studio, albums)</td>\n","      <td>16.852530</td>\n","      <td>band studio albums</td>\n","      <td>[band studio albums]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>643</th>\n","      <td>(weather, terminology)</td>\n","      <td>5.104337</td>\n","      <td>weather terminology</td>\n","      <td>[weather terminology]</td>\n","    </tr>\n","    <tr>\n","      <th>644</th>\n","      <td>(states, presidential)</td>\n","      <td>4.841302</td>\n","      <td>states presidential</td>\n","      <td>[states presidential]</td>\n","    </tr>\n","    <tr>\n","      <th>645</th>\n","      <td>(winter, storm)</td>\n","      <td>4.841302</td>\n","      <td>winter storm</td>\n","      <td>[winter storm]</td>\n","    </tr>\n","    <tr>\n","      <th>646</th>\n","      <td>(severe, weather)</td>\n","      <td>4.519374</td>\n","      <td>severe weather</td>\n","      <td>[severe weather]</td>\n","    </tr>\n","    <tr>\n","      <th>647</th>\n","      <td>(climate, united)</td>\n","      <td>4.033947</td>\n","      <td>climate united</td>\n","      <td>[climate united]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>648 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a891744-ad0d-4184-a0d6-cd81312d3f2c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7a891744-ad0d-4184-a0d6-cd81312d3f2c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7a891744-ad0d-4184-a0d6-cd81312d3f2c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":194}],"source":["import nltk\n","bigrams = nltk.collocations.BigramAssocMeasures()\n","trigrams = nltk.collocations.TrigramAssocMeasures()\n","bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(new_tokens)\n","trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(new_tokens)\n","\n","#bigrams\n","bigram_freq = bigramFinder.ngram_fd.items()\n","bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n","#trigrams\n","trigram_freq = trigramFinder.ngram_fd.items()\n","trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n","\n","#get english stopwords\n","stopwords_en = set(stopwords.words('english'))\n","\n","        #JJ  - adjective ‘cheap’ \n","        #JJR  - adjective, comparative ‘cheaper’ \n","        #JJS  - adjective, superlative ‘cheapest’\n","        #NN   - noun, singular ‘table’ \n","        #NNS  - noun plural ‘undergraduates’ \n","        #NNP  - proper noun, singular ‘Rohan' \n","        #NNPS - proper noun, plural ‘Indians’ \n","\n","        #Bigrams: (Noun, Noun), (Adjective, Noun)\n","        #Trigrams: (Adjective/Noun, Anything, Adjective/Noun)\n","\n","#function to filter for ADJ/NN bigrams\n","def rightTypes(ngram):\n","    if '-pron-' in ngram or 't' in ngram:\n","        return False\n","    for word in ngram:\n","        if word in stopwords_en or word.isspace():\n","            return False\n","    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n","    second_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n","    tags = nltk.pos_tag(ngram)\n","    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n","        return True\n","    else:\n","        return False\n","\n","#function to filter for trigrams\n","def rightTypesTri(ngram):\n","    if '-pron-' in ngram or 't' in ngram:\n","        return False\n","    for word in ngram:\n","        if word in stopwords_en or word.isspace():\n","            return False\n","    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n","    second_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS') ########\n","    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n","    tags = nltk.pos_tag(ngram)\n","    if tags[0][1] in first_type and tags[1][1] in second_type and tags[2][1] in third_type:\n","        return True\n","    else:\n","        return False\n","\n","#filter bigrams\n","filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda new_tokens: rightTypes(new_tokens))]\n","#filter trigrams\n","filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda new_tokens: rightTypesTri(new_tokens))]\n","\n","\n","#filter for only those with more than 2 occurences\n","bigramPMITable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.pmi)), columns=['candidate','PMI']).sort_values(by='PMI', ascending=False)\n","trigramPMITable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.pmi)), columns=['candidate','PMI']).sort_values(by='PMI', ascending=False)\n","\n","candidates = pd.concat([bigramPMITable, trigramPMITable]).sort_values(by='PMI', ascending=False)\n","print(len(candidates)) ##\n","\n","candidates = candidates[(candidates['PMI']>4)]\n","\n","list_candidate = []\n","string_candidate = []\n","for candidate in candidates.candidate:\n","  list_unit = []\n","  ngram = \" \".join(candidate)\n","  list_unit.append(ngram)\n","  list_candidate.append(list_unit)\n","  string_candidate.append(ngram)\n","\n","candidates[\"candidates_string\"] = string_candidate\n","candidates[\"candidates_list\"] = list_candidate\n","candidates.reset_index(drop=True, inplace=True)\n","candidates "]},{"cell_type":"markdown","metadata":{"id":"xdvzJeSQgMTv"},"source":["#Rótulo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHPCaFALM5QW"},"outputs":[],"source":["def calculate_cosine_distance(a, b):\n","    cosine_distance = float(spatial.distance.cosine(a, b))\n","    return cosine_distance\n","\n","def calculate_cosine_similarity(a, b):\n","    cosine_similarity = 1 - calculate_cosine_distance(a, b)\n","    return cosine_similarity\n","\n","def calculate_angular_distance(a, b):\n","    cosine_similarity = calculate_cosine_similarity(a, b)\n","    angular_distance = math.acos(cosine_similarity) / math.pi\n","    return angular_distance\n","\n","def calculate_angular_similarity(a, b):\n","    angular_similarity = 1 - calculate_angular_distance(a, b)\n","    return angular_similarity\n","\n","def distance(v1, v2):\n","    return np.sqrt(np.sum((v1 - v2) ** 2))  \n","\n","def minmax_norm(df_input):\n","    return (df_input - df_input.min()) / ( df_input.max() - df_input.min()) \n","\n","def word_mover_similarity(a, b):\n","    # your distance score needs to be converted to a similarity score\n","    similarity = TODO_IMPLEMENT(a, b)\n","    return similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X29_-0fRTTjg","executionInfo":{"status":"ok","timestamp":1675448272934,"user_tz":180,"elapsed":3618,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7b0e231a-5ddc-400c-c0ce-964ee81d1c70"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/scipy/spatial/distance.py:699: RuntimeWarning: invalid value encountered in double_scalars\n","  dist = 1.0 - uv / np.sqrt(uu * vv)\n","/usr/local/lib/python3.8/dist-packages/scipy/spatial/distance.py:699: RuntimeWarning: invalid value encountered in float_scalars\n","  dist = 1.0 - uv / np.sqrt(uu * vv)\n"]}],"source":["name = ''\n","dic_cand = {} #soma das distancias de cada palavra para calcular com os tokens\n","# (x + y + z)/3 dist a\n","\n","for n in range(0, 10):\n","  name = 'Topic '+ str(n)\n","  dic_cand[name] = {}\n","  dic_cand[name]['cosine'] = {}\n","  med_cos = {}\n","\n","  for c in candidates.candidate:\n","    test = 0\n","    cal_cos = []\n","    for i in range(len(c)):\n","      primeiro = small_model[c[i]]\n","      test += primeiro\n","    media = test/len(c)\n","\n","    for t in df_topic[name]:\n","      array_t = small_model[t]\n","      cos_sim = calculate_cosine_similarity(media, array_t)\n","      cal_cos.append(cos_sim)\n","\n","    sum_cos = 0\n","    for i in cal_cos:\n","      sum_cos += i\n","    med_cos[sum_cos/10] = c\n","    dic_cand[name]['cosine'] = med_cos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CghBLlQlVCsw"},"outputs":[],"source":["name = ''\n","dic_cand_filter = {}\n","\n","for n in range(0, 10):\n","  name = 'Topic '+ str(n)\n","  dic_cand_filter[name] = {}\n","  a = sorted(dic_cand[name]['cosine'].items(), reverse=True) #classificando os com valores maiores\n","  edit_a = a[:200]\n","  lista_name = []\n","\n","  for i in edit_a:\n","    lista_name.append(i[1])\n","  dic_cand_filter[name] = lista_name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGGzQ6xlVyud"},"outputs":[],"source":["name = ''\n","dic_cand_best = {}\n","\n","for n in range(10):\n","  name = 'Topic '+ str(n)\n","  dic_cand_best[name] = {}\n","  dic_cand_best[name]['cosine'] = {} \n","  dic_cand_best[name]['angular'] = {}\n","  dic_cand_best[name]['euclidean'] = {}\n","\n","  med_cos = {}\n","  med_ang = {}\n","  med_euclidean = {}\n","\n","  for c in dic_cand_filter[name]:\n","    test = 0\n","    for i in range(len(c)):\n","      primeiro = small_model[c[i]]\n","      test += primeiro\n","    media = test/len(c)\n","    str_c = ' '.join(c)\n","\n","    cal_cos = []\n","    cal_ang = []\n","    cal_euclidean = []\n","\n","    for t in df_topic[name]:\n","      array_t = small_model[t]\n","\n","      cos_sim = calculate_cosine_similarity(media, array_t)\n","      ang_sim = calculate_angular_similarity(array_t, media)\n","      euclidean = np.linalg.norm(array_t - media)\n","\n","      cal_cos.append(cos_sim)\n","      cal_ang.append(ang_sim)\n","      cal_euclidean.append(euclidean)\n","\n","    sum_cos = 0\n","    sum_jacc = 0\n","    sum_ang = 0\n","    sum_euclidean = 0\n","\n","    for i in cal_cos:\n","      sum_cos += i\n","    for j in cal_ang:\n","      sum_ang += j\n","    for w in cal_euclidean:\n","      sum_euclidean += w\n","\n","    med_cos[str_c] = sum_cos/10\n","    med_ang[str_c] = sum_ang/10\n","    med_euclidean[str_c] = sum_euclidean/10\n","\n","    dic_cand_best[name]['cosine'] = med_cos\n","    dic_cand_best[name]['cosine2'] = med_cos\n","    dic_cand_best[name]['angular'] = med_ang\n","    dic_cand_best[name]['euclidean'] = med_euclidean\n","\n","  df_cand_select = pd.DataFrame(data=dic_cand_best[name])\n","  df_cand_select['value'] = df_cand_select.sum(axis=1)/len(dic_cand_best[name].keys())\n","  df_cand_select = df_cand_select.reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AwvcOrNnFdHC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675448275258,"user_tz":180,"elapsed":15,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"}},"outputId":"effd30ed-3b2b-4afe-fa7a-8f56c421ad31"},"outputs":[{"output_type":"stream","name":"stdout","text":["Topic 0: reports, snow, inch, mph, wind, cocorahs, station, today, cst, biden\n","candidates: weather report metar\n","\n","\n","Topic 1: wordle, word, guess, streak, nyt, phew, nytimes, guesses, play, close\n","candidates: games google doodle\n","\n","\n","Topic 2: basketball, boys, girls, game, state, soccer, team, final, championship, tournament\n","candidates: college football playoff\n","\n","\n","Topic 3: inch, reports, freezing, snow, rain, cst, radio, amateur, sleet, public\n","candidates: ice storm weather\n","\n","\n","Topic 4: album, kanye, song, released, drop, songs, listen, listening, good, best\n","candidates: discography eminem music\n","\n","\n","Topic 5: poisoning, food, cheese, salad, sandwich, chicken, diarrhea, ate, vomiting, sick\n","candidates: banana bacon burger\n","\n","\n","Topic 6: vote, voting, putin, trump, republicans, gop, republican, russia, party, praising\n","candidates: government democratic presidential\n","\n","\n","Topic 7: artist, art, tattoo, nft, painting, work, artists, design, piece, project\n","candidates: paintings drawings\n","\n","\n","Topic 8: cocorahs, station, mst, inch, reports, snow, boulder, springs, montrose, ese\n","candidates: weather report metar\n","\n","\n","Topic 9: church, worship, pastor, join, bible, study, prayer, sunday, god, baptist\n","candidates: fellowship protestant liturgy\n","\n","\n"]}],"source":["dic_cand_final = pd.DataFrame()\n","\n","topic_num = []\n","topic_word = []\n","topic_label = []\n","\n","for n in range(0, 10):\n","  name = 'Topic '+ str(n)\n","  topic_num.append(name)\n","\n","\n","  df_cand_select = pd.DataFrame(data=dic_cand_best[name])\n","  df_cand_select['cosine2'] = df_cand_select['cosine']\n","  df_cand_select = minmax_norm(df_cand_select)\n","  df_cand_select['value'] = df_cand_select.sum(axis=1)/len(dic_cand_best[name].keys())\n","  df_cand_select = df_cand_select.reset_index()\n","\n","  best_med = 0\n","  cand_name = ''\n","  for i in range(len(df_cand_select['value'])):\n","      if df_cand_select['value'][i] > best_med:\n","        best_med = df_cand_select['value'][i]\n","        cand_name = df_cand_select['index'][i]\n","\n","  tokens_lda = []\n","  for t in df_topic[name]:\n","    tokens_lda.append(t)\n","\n","  topic_word.append(tokens_lda)\n","\n","  tokens_lda_str = \", \".join(tokens_lda)\n","  print(str(name) +\": \"+str(tokens_lda_str))\n","\n","  df_cand_select  = df_cand_select.sort_values(by='value', ascending=False).head(5)\n","  l = []\n","  for i in df_cand_select[:1]['index']:\n","    l.append(i)\n","    topic_label.append(i)\n","  l_str = \", \".join(l)\n","  print(\"candidates: \" +str(l_str))\n","  print(\"\\n\")\n","\n","dic_cand_final['Indice'] = topic_num\n","dic_cand_final['Tópico'] = topic_word\n","dic_cand_final['Rótulo'] = topic_label"]},{"cell_type":"code","source":["dic_cand_final"],"metadata":{"id":"U7iLrf-CnUJI","colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"status":"ok","timestamp":1675448275259,"user_tz":180,"elapsed":12,"user":{"displayName":"Annie Vianna Amorim","userId":"06883569788853818090"}},"outputId":"33b1a1b8-4a30-492e-c0ab-086c6a2ed239"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    Indice  \\\n","0  Topic 0   \n","1  Topic 1   \n","2  Topic 2   \n","3  Topic 3   \n","4  Topic 4   \n","5  Topic 5   \n","6  Topic 6   \n","7  Topic 7   \n","8  Topic 8   \n","9  Topic 9   \n","\n","                                                                                  Tópico  \\\n","0                 [reports, snow, inch, mph, wind, cocorahs, station, today, cst, biden]   \n","1                [wordle, word, guess, streak, nyt, phew, nytimes, guesses, play, close]   \n","2  [basketball, boys, girls, game, state, soccer, team, final, championship, tournament]   \n","3              [inch, reports, freezing, snow, rain, cst, radio, amateur, sleet, public]   \n","4             [album, kanye, song, released, drop, songs, listen, listening, good, best]   \n","5     [poisoning, food, cheese, salad, sandwich, chicken, diarrhea, ate, vomiting, sick]   \n","6    [vote, voting, putin, trump, republicans, gop, republican, russia, party, praising]   \n","7            [artist, art, tattoo, nft, painting, work, artists, design, piece, project]   \n","8         [cocorahs, station, mst, inch, reports, snow, boulder, springs, montrose, ese]   \n","9            [church, worship, pastor, join, bible, study, prayer, sunday, god, baptist]   \n","\n","                               Rótulo  \n","0                weather report metar  \n","1                 games google doodle  \n","2            college football playoff  \n","3                   ice storm weather  \n","4            discography eminem music  \n","5                 banana bacon burger  \n","6  government democratic presidential  \n","7                  paintings drawings  \n","8                weather report metar  \n","9       fellowship protestant liturgy  "],"text/html":["\n","  <div id=\"df-db574b2b-3b0a-4cd0-9f69-3978d4afb689\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Indice</th>\n","      <th>Tópico</th>\n","      <th>Rótulo</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Topic 0</td>\n","      <td>[reports, snow, inch, mph, wind, cocorahs, station, today, cst, biden]</td>\n","      <td>weather report metar</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Topic 1</td>\n","      <td>[wordle, word, guess, streak, nyt, phew, nytimes, guesses, play, close]</td>\n","      <td>games google doodle</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Topic 2</td>\n","      <td>[basketball, boys, girls, game, state, soccer, team, final, championship, tournament]</td>\n","      <td>college football playoff</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Topic 3</td>\n","      <td>[inch, reports, freezing, snow, rain, cst, radio, amateur, sleet, public]</td>\n","      <td>ice storm weather</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Topic 4</td>\n","      <td>[album, kanye, song, released, drop, songs, listen, listening, good, best]</td>\n","      <td>discography eminem music</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Topic 5</td>\n","      <td>[poisoning, food, cheese, salad, sandwich, chicken, diarrhea, ate, vomiting, sick]</td>\n","      <td>banana bacon burger</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Topic 6</td>\n","      <td>[vote, voting, putin, trump, republicans, gop, republican, russia, party, praising]</td>\n","      <td>government democratic presidential</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Topic 7</td>\n","      <td>[artist, art, tattoo, nft, painting, work, artists, design, piece, project]</td>\n","      <td>paintings drawings</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Topic 8</td>\n","      <td>[cocorahs, station, mst, inch, reports, snow, boulder, springs, montrose, ese]</td>\n","      <td>weather report metar</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Topic 9</td>\n","      <td>[church, worship, pastor, join, bible, study, prayer, sunday, god, baptist]</td>\n","      <td>fellowship protestant liturgy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db574b2b-3b0a-4cd0-9f69-3978d4afb689')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-db574b2b-3b0a-4cd0-9f69-3978d4afb689 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-db574b2b-3b0a-4cd0-9f69-3978d4afb689');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":200}]}],"metadata":{"colab":{"provenance":[{"file_id":"1HBuXX51dYk2ChearHleqzXIP_DvdyUFr","timestamp":1666709072434},{"file_id":"1iNXR5TLJOLnV3MwDfY-8EcdIqYX5I-26","timestamp":1666701372621},{"file_id":"1TWQVpeg7X68zgwWxhuk3j4ZHRSqdX1fV","timestamp":1666700553478},{"file_id":"156BIQbQeYzubqL21SPp1nVIeWfJH0SOE","timestamp":1651556580956}],"collapsed_sections":["9R2SrshvaWgK","w-DXNJ5uZw7U","O5c4YZ0IgDXB","xdvzJeSQgMTv"],"authorship_tag":"ABX9TyNmAR6N1IQ2ysFs7s2/hzrp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}