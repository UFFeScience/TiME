{"cells":[{"cell_type":"markdown","metadata":{"id":"69KhUlffMr5e"},"source":["#Entrada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4qa9eh_g6cz"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuK-stQ8rVpq"},"outputs":[],"source":["# For sending GET requests from the API\n","import requests\n","# For saving access tokens and for file management when creating and adding to the dataset\n","import os\n","# For dealing with json responses we receive from the API\n","import json\n","# For displaying the data after\n","import pandas as pd\n","# For saving the response data in CSV format\n","import csv\n","# For parsing the dates received from twitter in readable formats\n","import datetime\n","import dateutil.parser\n","import unicodedata\n","# To add wait time between requests\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsRUWPoorahG"},"outputs":[],"source":["os.environ['TOKEN'] = '...token...'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6_5a72Trjff"},"outputs":[],"source":["def auth():\n","    return os.getenv('TOKEN')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fp7cIZ7zrmnC"},"outputs":[],"source":["def create_headers(bearer_token):\n","    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n","    return headers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVnR8C10ro8G"},"outputs":[],"source":["def create_url(keyword, start_date, end_date, max_results = 10):\n","    \n","    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n","\n","    #change params based on the endpoint you are using\n","    query_params = {'query': keyword,\n","                    'start_time': start_date,\n","                    'end_time': end_date,\n","                    'max_results': max_results,\n","                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n","                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n","                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n","                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n","                    'next_token': {}}\n","    return (search_url, query_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8nK7aUZrvR9"},"outputs":[],"source":["def connect_to_endpoint(url, headers, params, next_token = None):\n","    params['next_token'] = next_token   #params object received from create_url function\n","    response = requests.request(\"GET\", url, headers = headers, params = params)\n","    print(\"Endpoint Response Code: \" + str(response.status_code))\n","    if response.status_code != 200:\n","        raise Exception(response.status_code, response.text)\n","    return response.json()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xM4co9MMs6xf"},"outputs":[],"source":["def append_to_csv(json_response, fileName):\n","\n","    #A counter variable\n","    counter = 0\n","\n","    #Open OR create the target CSV file\n","    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n","    csvWriter = csv.writer(csvFile)\n","\n","    #Loop through each tweet\n","    for tweet in json_response['data']:\n","        \n","        # We will create a variable for each since some of the keys might not exist for some tweets\n","        # So we will account for that\n","\n","        # 1. Author ID\n","        author_id = tweet['author_id']\n","\n","        # 2. Time created\n","        created_at = dateutil.parser.parse(tweet['created_at'])\n","\n","        # 3. Geolocation\n","        if ('geo' in tweet):   \n","            geo = tweet['geo']['place_id']\n","        else:\n","            geo = \" \"\n","\n","        # 4. Tweet ID\n","        tweet_id = tweet['id']\n","\n","        # 5. Language\n","        lang = tweet['lang']\n","\n","        # 6. Tweet metrics\n","        retweet_count = tweet['public_metrics']['retweet_count']\n","        reply_count = tweet['public_metrics']['reply_count']\n","        like_count = tweet['public_metrics']['like_count']\n","        quote_count = tweet['public_metrics']['quote_count']\n","\n","        # 7. source\n","        source = tweet['source']\n","\n","        # 8. Tweet text\n","        text = tweet['text']\n","        \n","        # Assemble all data in a list\n","        res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n","        \n","        # Append the result to the CSV file\n","        csvWriter.writerow(res)\n","        counter += 1\n","\n","    # When done, close the CSV file\n","    csvFile.close()\n","\n","    # Print the number of tweets for this iteration\n","    print(\"# of Tweets added from this response: \", counter) "]},{"cell_type":"markdown","metadata":{"id":"Mwj1B0IYs9DR"},"source":["#Tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpctaZZzs9DS"},"outputs":[],"source":["days = [\"%.2d\" % i for i in range(1,28)]\n","tomorrow = [\"%.2d\" % i for i in range(2,29)]\n","\n","fevereiro = {}\n","keyword = \"(Snow OR Wind OR Rain OR Storm OR Sunny OR Rainbow OR Cloudy OR Blizzard OR Tornado OR Thunderstorm) place_country:US lang:en -is:retweet -is:reply -is:quote\"\n","\n","for day in range(0,27):\n","  #Inputs for tweets\n","  bearer_token = auth()\n","  headers = create_headers(bearer_token)\n","\n","  start_list =    ['2022-02-'+days[day]+'T00:00:00.000Z',\n","                  '2022-02-'+days[day]+'T01:00:00.000Z',\n","                  '2022-02-'+days[day]+'T02:00:00.000Z',\n","                  '2022-02-'+days[day]+'T03:00:00.000Z',\n","                  '2022-02-'+days[day]+'T04:00:00.000Z',\n","                  '2022-02-'+days[day]+'T05:00:00.000Z',\n","                  '2022-02-'+days[day]+'T06:00:00.000Z',\n","                  '2022-02-'+days[day]+'T07:00:00.000Z',\n","                  '2022-02-'+days[day]+'T08:00:00.000Z',\n","                  '2022-02-'+days[day]+'T09:00:00.000Z',\n","                  '2022-02-'+days[day]+'T10:00:00.000Z',\n","                  '2022-02-'+days[day]+'T11:00:00.000Z',\n","                  '2022-02-'+days[day]+'T12:00:00.000Z',\n","                  '2022-02-'+days[day]+'T13:00:00.000Z',\n","                  '2022-02-'+days[day]+'T14:00:00.000Z',\n","                  '2022-02-'+days[day]+'T15:00:00.000Z',\n","                  '2022-02-'+days[day]+'T16:00:00.000Z',\n","                  '2022-02-'+days[day]+'T17:00:00.000Z',\n","                  '2022-02-'+days[day]+'T18:00:00.000Z',\n","                  '2022-02-'+days[day]+'T19:00:00.000Z',\n","                  '2022-02-'+days[day]+'T20:00:00.000Z',\n","                  '2022-02-'+days[day]+'T21:00:00.000Z',\n","                  '2022-02-'+days[day]+'T22:00:00.000Z',\n","                  '2022-02-'+days[day]+'T23:00:00.000Z',\n","                  ]\n","\n","  end_list =      ['2022-02-'+days[day]+'T01:00:00.000Z',\n","                  '2022-02-'+days[day]+'T02:00:00.000Z',\n","                  '2022-02-'+days[day]+'T03:00:00.000Z',\n","                  '2022-02-'+days[day]+'T04:00:00.000Z',\n","                  '2022-02-'+days[day]+'T05:00:00.000Z',\n","                  '2022-02-'+days[day]+'T06:00:00.000Z',\n","                  '2022-02-'+days[day]+'T07:00:00.000Z',\n","                  '2022-02-'+days[day]+'T08:00:00.000Z',\n","                  '2022-02-'+days[day]+'T09:00:00.000Z',\n","                  '2022-02-'+days[day]+'T10:00:00.000Z',\n","                  '2022-02-'+days[day]+'T11:00:00.000Z',\n","                  '2022-02-'+days[day]+'T12:00:00.000Z',\n","                  '2022-02-'+days[day]+'T13:00:00.000Z',\n","                  '2022-02-'+days[day]+'T14:00:00.000Z',\n","                  '2022-02-'+days[day]+'T15:00:00.000Z',\n","                  '2022-02-'+days[day]+'T16:00:00.000Z',\n","                  '2022-02-'+days[day]+'T17:00:00.000Z',\n","                  '2022-02-'+days[day]+'T18:00:00.000Z',\n","                  '2022-02-'+days[day]+'T19:00:00.000Z',\n","                  '2022-02-'+days[day]+'T20:00:00.000Z',\n","                  '2022-02-'+days[day]+'T21:00:00.000Z',\n","                  '2022-02-'+days[day]+'T22:00:00.000Z',\n","                  '2022-02-'+days[day]+'T23:00:00.000Z',\n","                  '2022-02-'+tomorrow[day]+'T00:00:00.000Z']\n","   \n","\n","  max_results = 500\n","\n","  #Total number of tweets we collected from the loop\n","  total_tweets = 0\n","\n","  # Create file\n","  csvFile = open(\"data_22_02_\"+days[day]+\".csv\", \"a\", newline=\"\", encoding='utf-8')\n","  csvWriter = csv.writer(csvFile)\n","\n","  #Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n","  csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n","  csvFile.close()\n","\n","  for i in range(0,len(start_list)):\n","\n","      # Inputs\n","      count = 0 # Counting tweets per time period\n","      max_count = 100 # Max tweets per time period\n","      flag = True\n","      next_token = None\n","      \n","      # Check if flag is true\n","      while flag:\n","          # Check if max_count reached\n","          if count >= max_count:\n","              break\n","          print(\"-------------------\")\n","          print(\"Token: \", next_token)\n","          url = create_url(keyword, start_list[i],end_list[i], max_results)\n","          json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n","          result_count = json_response['meta']['result_count']\n","\n","          if 'next_token' in json_response['meta']:\n","              # Save the token to use for next call\n","              next_token = json_response['meta']['next_token']\n","              print(\"Next Token: \", next_token)\n","              if result_count is not None and result_count > 0 and next_token is not None:\n","                  print(\"Start Date: \", start_list[i])\n","                  append_to_csv(json_response, \"data_22_02_\"+days[day]+\".csv\")\n","                  count += result_count\n","                  total_tweets += result_count\n","                  print(\"Total # of Tweets added: \", total_tweets)\n","                  print(\"-------------------\")\n","                  time.sleep(5)                \n","          # If no next token exists\n","          else:\n","              if result_count is not None and result_count > 0:\n","                  print(\"-------------------\")\n","                  print(\"Start Date: \", start_list[i])\n","                  append_to_csv(json_response, \"data_22_02_\"+days[day]+\".csv\")\n","                  count += result_count\n","                  total_tweets += result_count\n","                  print(\"Total # of Tweets added: \", total_tweets)\n","                  print(\"-------------------\")\n","                  time.sleep(5)\n","              \n","              #Since this is the final request, turn flag to false to move to the next time period.\n","              flag = False\n","              next_token = None\n","          time.sleep(5)\n","\n","  print('\\n')\n","  print(\"******************\")\n","  print('2022-02-'+ days[day])\n","  fevereiro_data = '2022-02-'+ days[day]\n","  fevereiro[fevereiro_data] = total_tweets\n","  print(\"Total number of results: \", total_tweets)\n","  print(\"******************\")\n","  print('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ro-bQRwxs9DT"},"outputs":[],"source":["days = [\"%.2d\" % i for i in range(28,29)]\n","tomorrow = [\"%.2d\" % i for i in range(1,2)]\n","\n","\n","for day in range(0,1):\n","  #Inputs for tweets\n","  bearer_token = auth()\n","  headers = create_headers(bearer_token)\n","\n","  start_list =    ['2022-02-'+days[day]+'T00:00:00.000Z',\n","                  '2022-02-'+days[day]+'T01:00:00.000Z',\n","                  '2022-02-'+days[day]+'T02:00:00.000Z',\n","                  '2022-02-'+days[day]+'T03:00:00.000Z',\n","                  '2022-02-'+days[day]+'T04:00:00.000Z',\n","                  '2022-02-'+days[day]+'T05:00:00.000Z',\n","                  '2022-02-'+days[day]+'T06:00:00.000Z',\n","                  '2022-02-'+days[day]+'T07:00:00.000Z',\n","                  '2022-02-'+days[day]+'T08:00:00.000Z',\n","                  '2022-02-'+days[day]+'T09:00:00.000Z',\n","                  '2022-02-'+days[day]+'T10:00:00.000Z',\n","                  '2022-02-'+days[day]+'T11:00:00.000Z',\n","                  '2022-02-'+days[day]+'T12:00:00.000Z',\n","                  '2022-02-'+days[day]+'T13:00:00.000Z',\n","                  '2022-02-'+days[day]+'T14:00:00.000Z',\n","                  '2022-02-'+days[day]+'T15:00:00.000Z',\n","                  '2022-02-'+days[day]+'T16:00:00.000Z',\n","                  '2022-02-'+days[day]+'T17:00:00.000Z',\n","                  '2022-02-'+days[day]+'T18:00:00.000Z',\n","                  '2022-02-'+days[day]+'T19:00:00.000Z',\n","                  '2022-02-'+days[day]+'T20:00:00.000Z',\n","                  '2022-02-'+days[day]+'T21:00:00.000Z',\n","                  '2022-02-'+days[day]+'T22:00:00.000Z',\n","                  '2022-02-'+days[day]+'T23:00:00.000Z',\n","                  ]\n","\n","  end_list =      ['2022-02-'+days[day]+'T01:00:00.000Z',\n","                  '2022-02-'+days[day]+'T02:00:00.000Z',\n","                  '2022-02-'+days[day]+'T03:00:00.000Z',\n","                  '2022-02-'+days[day]+'T04:00:00.000Z',\n","                  '2022-02-'+days[day]+'T05:00:00.000Z',\n","                  '2022-02-'+days[day]+'T06:00:00.000Z',\n","                  '2022-02-'+days[day]+'T07:00:00.000Z',\n","                  '2022-02-'+days[day]+'T08:00:00.000Z',\n","                  '2022-02-'+days[day]+'T09:00:00.000Z',\n","                  '2022-02-'+days[day]+'T10:00:00.000Z',\n","                  '2022-02-'+days[day]+'T11:00:00.000Z',\n","                  '2022-02-'+days[day]+'T12:00:00.000Z',\n","                  '2022-02-'+days[day]+'T13:00:00.000Z',\n","                  '2022-02-'+days[day]+'T14:00:00.000Z',\n","                  '2022-02-'+days[day]+'T15:00:00.000Z',\n","                  '2022-02-'+days[day]+'T16:00:00.000Z',\n","                  '2022-02-'+days[day]+'T17:00:00.000Z',\n","                  '2022-02-'+days[day]+'T18:00:00.000Z',\n","                  '2022-02-'+days[day]+'T19:00:00.000Z',\n","                  '2022-02-'+days[day]+'T20:00:00.000Z',\n","                  '2022-02-'+days[day]+'T21:00:00.000Z',\n","                  '2022-02-'+days[day]+'T22:00:00.000Z',\n","                  '2022-02-'+days[day]+'T23:00:00.000Z',\n","                  '2022-03-'+tomorrow[day]+'T00:00:00.000Z']\n","\n","  max_results = 500\n","\n","  #Total number of tweets we collected from the loop\n","  total_tweets = 0\n","\n","  # Create file\n","  csvFile = open(\"data_22_02_\"+days[day]+\".csv\", \"a\", newline=\"\", encoding='utf-8')\n","  csvWriter = csv.writer(csvFile)\n","\n","  #Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n","  csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n","  csvFile.close()\n","\n","  for i in range(0,len(start_list)):\n","\n","      # Inputs\n","      count = 0 # Counting tweets per time period\n","      max_count = 100 # Max tweets per time period\n","      flag = True\n","      next_token = None\n","      \n","      # Check if flag is true\n","      while flag:\n","          # Check if max_count reached\n","          if count >= max_count:\n","              break\n","          print(\"-------------------\")\n","          print(\"Token: \", next_token)\n","          url = create_url(keyword, start_list[i],end_list[i], max_results)\n","          json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n","          result_count = json_response['meta']['result_count']\n","\n","          if 'next_token' in json_response['meta']:\n","              # Save the token to use for next call\n","              next_token = json_response['meta']['next_token']\n","              print(\"Next Token: \", next_token)\n","              if result_count is not None and result_count > 0 and next_token is not None:\n","                  print(\"Start Date: \", start_list[i])\n","                  append_to_csv(json_response, \"data_22_02_\"+days[day]+\".csv\")\n","                  count += result_count\n","                  total_tweets += result_count\n","                  print(\"Total # of Tweets added: \", total_tweets)\n","                  print(\"-------------------\")\n","                  time.sleep(5)                \n","          # If no next token exists\n","          else:\n","              if result_count is not None and result_count > 0:\n","                  print(\"-------------------\")\n","                  print(\"Start Date: \", start_list[i])\n","                  append_to_csv(json_response, \"data_22_02_\"+days[day]+\".csv\")\n","                  count += result_count\n","                  total_tweets += result_count\n","                  print(\"Total # of Tweets added: \", total_tweets)\n","                  print(\"-------------------\")\n","                  time.sleep(5)\n","              \n","              #Since this is the final request, turn flag to false to move to the next time period.\n","              flag = False\n","              next_token = None\n","          time.sleep(5)\n","\n","  print('\\n')\n","  print(\"******************\")\n","  print('2022-02-'+days[day])\n","  fevereiro_data = '2022-02-'+days[day]\n","  fevereiro[fevereiro_data] = total_tweets\n","  print(\"Total number of results: \", total_tweets)\n","  print(\"******************\")\n","  print('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZnf6U_Fs9DV"},"outputs":[],"source":["fevereiro_df = pd.DataFrame(list(fevereiro.items()), columns=['data', 'tweets'])\n","fevereiro_df"]}],"metadata":{"colab":{"provenance":[{"file_id":"1KoE2h_gR-J4_W9gOrWRhrksPPeiyMv0s","timestamp":1657204647170},{"file_id":"1YXIOTb4ykr0gXeOz6Dgu4zbMGmTvtv6G","timestamp":1657202854036},{"file_id":"1bZ-VXCpBZVGHAMt4ZbgXhn5B6hu3B-C7","timestamp":1657152044143},{"file_id":"1AL4mU1qv6I7i9nsK9Dlrzw8KR0ZDbxOZ","timestamp":1656338768473},{"file_id":"1HV4yncapTYS629L4OcuoOeRlAHDM50KA","timestamp":1656080708014},{"file_id":"1nUM2HEo-kDQSGMOWXWSoReAES4ibFs6y","timestamp":1656080432896},{"file_id":"13BlcNsDm4Z_BlPG4WObyrkjlJ1GlGP7w","timestamp":1655925848610},{"file_id":"1aAnS4hFbm-7zy0Z-_Xbp2G5QxwjOhQqI","timestamp":1655925838014},{"file_id":"1BqBrW7cDKTdMxSPN6rI0R46vw8ty5G_t","timestamp":1644149154916}],"authorship_tag":"ABX9TyONwQTnO+g3ihgp2OOzTvFB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}